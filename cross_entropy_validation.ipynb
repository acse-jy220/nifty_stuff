{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5d8b4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindformers.core.loss import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e524bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss1 = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80c406a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.numpy as msnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24ac47ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = msnp.randn(992, 30523)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6dd3d073",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = msnp.randn(992).astype(ms.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa9c8d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore as ms\n",
    "ms.set_context(device_id=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bdd4ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ms.load_checkpoint(\"/home/yj/QFormer/shifted_prediction_scores.ckpt\")['shifted_prediction_scores'].value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c36c713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(992, 30523)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cf4b756",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ms.load_checkpoint(\"/home/yj/QFormer/labels.ckpt\")['labels'].value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10e95089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(992,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71222322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.ops as ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39440994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.ops.operations as P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e28080c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_gather_d = P.GatherD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6227342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dim = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d90eb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = y.expand_dims(target_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9cff701",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = _gather_d(x, target_dim, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9be9e127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[992, 1], dtype=Float32, value=\n",
       "[[ 9.77265418e-01],\n",
       " [ 6.47866130e-01],\n",
       " [ 7.58817554e-01],\n",
       " ...\n",
       " [ 1.80141628e-02],\n",
       " [-6.06357396e-01],\n",
       " [ 5.73751628e-02]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "afd42639",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_x = ops.arange(y.shape[0]) * x.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9850c2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mod = ops.floor_mod(y, 30523)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c8e8d350",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mod = y_x + y_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d549db1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss2 = x.flatten()[y_mod].expand_dims(target_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d4fc0a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[992, 1], dtype=Float32, value=\n",
       "[[ 9.77265418e-01],\n",
       " [ 6.47866130e-01],\n",
       " [ 7.58817554e-01],\n",
       " ...\n",
       " [ 1.80141628e-02],\n",
       " [-6.06357396e-01],\n",
       " [ 5.73751628e-02]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b33ec6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.nn as nn\n",
    "import mindspore.numpy as msnp\n",
    "class CrossEntropyLossV2(nn.Cell):\n",
    "    \"\"\"\n",
    "    Calculate the cross entropy loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, target_dim=-1, weight=None, ignore_index=-100, reduction='mean', label_smoothing=0.0):\n",
    "        super(CrossEntropyLossV2, self).__init__()   \n",
    "        self.target_dim = target_dim\n",
    "        self.weight = weight\n",
    "        self.ignore_index = ignore_index\n",
    "        self.reduction = reduction\n",
    "        self.label_smoothing = label_smoothing\n",
    "        \n",
    "        self.exp = P.Exp()\n",
    "        self.log = P.Log()\n",
    "        self.neg = P.Neg()\n",
    "        self.gather = P.Gather()\n",
    "        self.ones_like = P.OnesLike()\n",
    "        self.equal = P.Equal()\n",
    "\n",
    "\n",
    "    def logsumexp(self, x, axis, keep_dims=False):\n",
    "        \"\"\"\n",
    "        Reduces a dimension of a tensor by calculating exponential for all elements in the dimension,\n",
    "        then calculate logarithm of the sum.\n",
    "        \"\"\"\n",
    "        _reduce_sum = P.ReduceSum(keep_dims)\n",
    "\n",
    "        x_max = x.max(axis=axis, keepdims=True)\n",
    "        x_exp = self.exp(x - x_max)\n",
    "        x_sumexp = _reduce_sum(x_exp, axis)\n",
    "        x_logsumexp = self.log(x_sumexp)\n",
    "        if not keep_dims:\n",
    "            x_max = x_max.squeeze(axis=axis)\n",
    "        return x_logsumexp + x_max\n",
    "\n",
    "    def log_softmax(self, inputs, axis):\n",
    "        \"\"\"inner implementation of log_softmax, since the LogSoftmaxGrad op do not support inputs > 2d\"\"\"\n",
    "        return inputs - self.logsumexp(inputs, axis, True)\n",
    "\n",
    "# def nll_loss(inputs, target, weight=None, ignore_index=-100, reduction='mean', label_smoothing=0.0):\n",
    "#     \"\"\"\n",
    "#     Gets the negative log likelihood loss between inputs and target.\n",
    "#     \"\"\"\n",
    "#     ndim = inputs.ndim\n",
    "#     if ndim == 2:\n",
    "#         ret = _nll_loss(inputs, target, -1, weight, ignore_index, reduction, label_smoothing)\n",
    "#     elif ndim == 4:\n",
    "#         ret = _nll_loss(inputs, target, 1, weight, ignore_index, reduction, label_smoothing)\n",
    "#     elif ndim == 1:\n",
    "#         ret = _nll_loss(inputs, target, 0, weight, ignore_index, reduction, label_smoothing)\n",
    "#     else:\n",
    "#         n = inputs.shape[0]\n",
    "#         c = inputs.shape[1]\n",
    "#         out_size = (n,) + inputs.shape[2:]\n",
    "#         inputs = inputs.view(n, c, 1, -1)\n",
    "#         target = target.view(n, 1, -1)\n",
    "#         if reduction != 'none':\n",
    "#             ret = _nll_loss(inputs, target, 1, weight, ignore_index, reduction, label_smoothing)\n",
    "#         else:\n",
    "#             ret = _nll_loss(inputs, target, 1, weight, ignore_index, label_smoothing=label_smoothing)\n",
    "#             ret = ret.view(out_size)\n",
    "#     return ret\n",
    "\n",
    "    def Gather_D(self, inputs, target_dim, target):\n",
    "        \"\"\"\n",
    "        Rewrite P.GatherD(), align with it.\n",
    "        \"\"\"\n",
    "        pred_x = msnp.arange(target.shape[0]) * inputs.shape[-1]\n",
    "        pred_mod = ops.floor_mod(target, inputs.shape[-1])\n",
    "        pred_idx = pred_x + pred_mod\n",
    "        return (inputs.flatten())[pred_idx].expand_dims(target_dim)\n",
    "\n",
    "    def nll_loss(self, inputs, target, target_dim=-1, weight=None, ignore_index=None, reduction='none', label_smoothing=0.0):\n",
    "        \"\"\"nll loss inner function\"\"\"\n",
    "        if target.ndim == inputs.ndim - 1:\n",
    "            target = target.expand_dims(target_dim)\n",
    "        if ignore_index is not None:\n",
    "            non_pad_mask = self.equal(target, ignore_index)\n",
    "            target = target.masked_fill(non_pad_mask, 0)\n",
    "        else:\n",
    "            non_pad_mask = target\n",
    "        target = target.squeeze(target_dim)\n",
    "        loss = self.neg(self.Gather_D(inputs, target_dim, target))\n",
    "        smooth_loss = self.neg(inputs.sum(axis=target_dim, keepdims=False))\n",
    "\n",
    "        if weight is not None:\n",
    "            loss_weights = self.gather(weight, target, 0)\n",
    "            loss = loss * loss_weights\n",
    "        else:\n",
    "            loss_weights = self.ones_like(loss)\n",
    "        if ignore_index is not None:\n",
    "            loss = loss.masked_fill(non_pad_mask, 0.)\n",
    "            loss_weights = loss_weights.masked_fill(non_pad_mask, 0.)\n",
    "    #         smooth_loss = smooth_loss.masked_fill(non_pad_mask, 0.)\n",
    "\n",
    "        loss = loss.squeeze(target_dim)\n",
    "    #     smooth_loss = smooth_loss.squeeze(target_dim)\n",
    "\n",
    "        if reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "            smooth_loss = smooth_loss.sum()\n",
    "        if reduction == 'mean':\n",
    "            loss = loss.sum() / loss_weights.sum()\n",
    "    #         loss = loss.mean()\n",
    "            smooth_loss = smooth_loss.mean()\n",
    "\n",
    "        loss = (1. - label_smoothing) * loss + label_smoothing * smooth_loss / inputs.shape[target_dim]\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def construct(self, inputs, target):\n",
    "        r\"\"\"\n",
    "        The cross entropy loss between input and target.\n",
    "        \"\"\"\n",
    "        class_dim = 0 if inputs.ndim == 1 else 1\n",
    "        log_softmax_result = self.log_softmax(inputs, class_dim)\n",
    "    #     print(\"log_softmax_result: \", log_softmax_result)\n",
    "        return self.nll_loss(log_softmax_result, target, self.target_dim, self.weight, self.ignore_index, self.reduction, self.label_smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9099c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_v2 = CrossEntropyLossV2(label_smoothing=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f826155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[], dtype=Float32, value= 10.2473)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_v2(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "43b1bb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import CrossEntropyLoss as torch_CrossEntropyLoss\n",
    "default_torch_crossentropy = torch_CrossEntropyLoss(label_smoothing=0.1)\n",
    "def compute_torch_CrossEntropy(x, y, torch_crossentropy=default_torch_crossentropy):\n",
    "    x_torch = torch.from_numpy(x.asnumpy())\n",
    "    y_torch = torch.from_numpy(y.asnumpy()).to(torch.long)\n",
    "    return torch_crossentropy(x_torch, y_torch).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6665d8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ms_loss: 10.7286 torch_loss: 10.7286 l1_error: 0.000004\n",
      "ms_loss: 10.6643 torch_loss: 10.6643 l1_error: 0.000058\n",
      "ms_loss: 10.7569 torch_loss: 10.7570 l1_error: 0.000021\n",
      "ms_loss: 10.7431 torch_loss: 10.7431 l1_error: 0.000030\n",
      "ms_loss: 10.7696 torch_loss: 10.7697 l1_error: 0.000065\n",
      "ms_loss: 10.6608 torch_loss: 10.6608 l1_error: 0.000011\n",
      "ms_loss: 10.7278 torch_loss: 10.7277 l1_error: 0.000037\n",
      "ms_loss: 10.7249 torch_loss: 10.7250 l1_error: 0.000076\n",
      "ms_loss: 10.7421 torch_loss: 10.7421 l1_error: 0.000014\n",
      "ms_loss: 10.7658 torch_loss: 10.7658 l1_error: 0.000007\n",
      "ms_loss: 10.7310 torch_loss: 10.7311 l1_error: 0.000044\n",
      "ms_loss: 10.7179 torch_loss: 10.7179 l1_error: 0.000015\n",
      "ms_loss: 10.7519 torch_loss: 10.7520 l1_error: 0.000045\n",
      "ms_loss: 10.8749 torch_loss: 10.8749 l1_error: 0.000014\n",
      "ms_loss: 10.6635 torch_loss: 10.6636 l1_error: 0.000085\n",
      "ms_loss: 10.6871 torch_loss: 10.6870 l1_error: 0.000029\n",
      "ms_loss: 10.7616 torch_loss: 10.7617 l1_error: 0.000101\n",
      "ms_loss: 10.7621 torch_loss: 10.7621 l1_error: 0.000047\n",
      "ms_loss: 10.7688 torch_loss: 10.7688 l1_error: 0.000043\n",
      "ms_loss: 10.7738 torch_loss: 10.7738 l1_error: 0.000029\n",
      "ms_loss: 10.8101 torch_loss: 10.8102 l1_error: 0.000018\n",
      "ms_loss: 10.7281 torch_loss: 10.7281 l1_error: 0.000089\n",
      "ms_loss: 10.8604 torch_loss: 10.8605 l1_error: 0.000020\n",
      "ms_loss: 10.7681 torch_loss: 10.7680 l1_error: 0.000058\n",
      "ms_loss: 10.8057 torch_loss: 10.8057 l1_error: 0.000044\n",
      "ms_loss: 10.7148 torch_loss: 10.7148 l1_error: 0.000051\n",
      "ms_loss: 10.8198 torch_loss: 10.8199 l1_error: 0.000052\n",
      "ms_loss: 10.6650 torch_loss: 10.6650 l1_error: 0.000044\n",
      "ms_loss: 10.6916 torch_loss: 10.6916 l1_error: 0.000056\n",
      "ms_loss: 10.7423 torch_loss: 10.7422 l1_error: 0.000016\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_447609/518950011.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mx_rand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmsnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0my_rand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmsnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mms_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_entropy_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx_rand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/archiconda3/envs/yj-1.10-py37/lib/python3.7/site-packages/mindspore/numpy/array_creations.py\u001b[0m in \u001b[0;36mrandn\u001b[0;34m(dtype, *shape)\u001b[0m\n\u001b[1;32m    458\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0mstdnormal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStandardNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mstdnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/archiconda3/envs/yj-1.10-py37/lib/python3.7/site-packages/mindspore/ops/primitive.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshould_elim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_run_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/archiconda3/envs/yj-1.10-py37/lib/python3.7/site-packages/mindspore/common/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*arg, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_convert_python_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/archiconda3/envs/yj-1.10-py37/lib/python3.7/site-packages/mindspore/ops/primitive.py\u001b[0m in \u001b[0;36m_run_op\u001b[0;34m(obj, op_name, args)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_run_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0;34m\"\"\"Single op execution function supported by ge in PyNative mode.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_run_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for i in range(round_n):\n",
    "    x_rand = msnp.randn(x.shape)\n",
    "    y_rand = msnp.randn(y.shape).astype(ms.int32)\n",
    "    ms_loss = cross_entropy_v2(x + x_rand, y).asnumpy()\n",
    "    torch_loss = compute_torch_CrossEntropy(x + x_rand, y)\n",
    "    print(f\"ms_loss: {ms_loss:.5f}\\b torch_loss: {torch_loss:.5f}\\b l1_error: {np.abs(ms_loss - torch_loss):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "717f4816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[], dtype=Float32, value= 10.2473)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy(x, y, label_smoothing=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "df2d5e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.ops.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "202ece31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_softmax_result:  [[-10.444187 -10.315205 -10.759834 ... -10.003607 -10.650483 -10.019769]\n",
      " [-10.113524 -11.206087 -10.332144 ... -10.155675 -10.70844  -10.906179]\n",
      " [-11.029074 -10.338369 -10.502667 ... -10.243991 -10.717099 -10.276416]\n",
      " ...\n",
      " [ -9.84885  -10.186969 -10.872633 ... -10.526638 -10.044142 -11.196835]\n",
      " [ -9.912016 -10.702992 -11.336755 ... -10.9684   -10.914676 -11.530894]\n",
      " [ -9.825527 -10.772986 -11.110288 ... -10.19183  -10.542184 -10.758883]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[], dtype=Float32, value= 10.2214)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(x, y, label_smoothing=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d8a59d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindformers.models.blip2.qformer import CrossEntropyLoss as CrossEntropyLoss_ori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfb194cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_ori = CrossEntropyLoss_ori(label_smoothing=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215cb642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ms_loss: 10.7461 ori_loss: 10.7461 l1_error: 0.000000\n",
      "ms_loss: 10.7775 ori_loss: 10.7775 l1_error: 0.000000\n",
      "ms_loss: 10.7705 ori_loss: 10.7705 l1_error: 0.000001\n",
      "ms_loss: 10.6944 ori_loss: 10.6944 l1_error: 0.000000\n",
      "ms_loss: 10.7715 ori_loss: 10.7715 l1_error: 0.000000\n",
      "ms_loss: 10.7896 ori_loss: 10.7896 l1_error: 0.000000\n",
      "ms_loss: 10.7053 ori_loss: 10.7053 l1_error: 0.000000\n",
      "ms_loss: 10.7394 ori_loss: 10.7394 l1_error: 0.000001\n",
      "ms_loss: 10.6897 ori_loss: 10.6897 l1_error: 0.000000\n",
      "ms_loss: 10.7667 ori_loss: 10.7667 l1_error: 0.000000\n",
      "ms_loss: 10.7316 ori_loss: 10.7316 l1_error: 0.000001\n",
      "ms_loss: 10.8103 ori_loss: 10.8103 l1_error: 0.000000\n",
      "ms_loss: 10.7917 ori_loss: 10.7917 l1_error: 0.000000\n",
      "ms_loss: 10.7400 ori_loss: 10.7400 l1_error: 0.000000\n",
      "ms_loss: 10.8030 ori_loss: 10.8030 l1_error: 0.000001\n",
      "ms_loss: 10.7240 ori_loss: 10.7240 l1_error: 0.000000\n",
      "ms_loss: 10.7224 ori_loss: 10.7224 l1_error: 0.000000\n",
      "ms_loss: 10.8546 ori_loss: 10.8546 l1_error: 0.000001\n",
      "ms_loss: 10.7472 ori_loss: 10.7472 l1_error: 0.000001\n",
      "ms_loss: 10.7569 ori_loss: 10.7569 l1_error: 0.000000\n",
      "ms_loss: 10.6960 ori_loss: 10.6960 l1_error: 0.000000\n",
      "ms_loss: 10.7484 ori_loss: 10.7484 l1_error: 0.000000\n",
      "ms_loss: 10.7046 ori_loss: 10.7046 l1_error: 0.000001\n",
      "ms_loss: 10.7665 ori_loss: 10.7665 l1_error: 0.000001\n",
      "ms_loss: 10.7163 ori_loss: 10.7163 l1_error: 0.000000\n",
      "ms_loss: 10.7917 ori_loss: 10.7917 l1_error: 0.000001\n",
      "ms_loss: 10.8096 ori_loss: 10.8096 l1_error: 0.000000\n",
      "ms_loss: 10.7856 ori_loss: 10.7856 l1_error: 0.000000\n",
      "ms_loss: 10.6930 ori_loss: 10.6930 l1_error: 0.000001\n",
      "ms_loss: 10.7711 ori_loss: 10.7711 l1_error: 0.000000\n",
      "ms_loss: 10.6485 ori_loss: 10.6485 l1_error: 0.000000\n",
      "ms_loss: 10.7050 ori_loss: 10.7050 l1_error: 0.000000\n",
      "ms_loss: 10.7543 ori_loss: 10.7543 l1_error: 0.000000\n",
      "ms_loss: 10.6940 ori_loss: 10.6940 l1_error: 0.000000\n",
      "ms_loss: 10.6775 ori_loss: 10.6775 l1_error: 0.000000\n",
      "ms_loss: 10.7597 ori_loss: 10.7597 l1_error: 0.000000\n",
      "ms_loss: 10.7169 ori_loss: 10.7169 l1_error: 0.000001\n",
      "ms_loss: 10.7458 ori_loss: 10.7458 l1_error: 0.000000\n",
      "ms_loss: 10.7853 ori_loss: 10.7853 l1_error: 0.000001\n",
      "ms_loss: 10.6765 ori_loss: 10.6765 l1_error: 0.000000\n",
      "ms_loss: 10.7998 ori_loss: 10.7998 l1_error: 0.000000\n",
      "ms_loss: 10.7604 ori_loss: 10.7604 l1_error: 0.000001\n",
      "ms_loss: 10.6599 ori_loss: 10.6599 l1_error: 0.000000\n",
      "ms_loss: 10.7425 ori_loss: 10.7425 l1_error: 0.000000\n",
      "ms_loss: 10.7670 ori_loss: 10.7670 l1_error: 0.000000\n",
      "ms_loss: 10.7429 ori_loss: 10.7429 l1_error: 0.000000\n",
      "ms_loss: 10.7882 ori_loss: 10.7882 l1_error: 0.000000\n",
      "ms_loss: 10.6903 ori_loss: 10.6903 l1_error: 0.000000\n",
      "ms_loss: 10.8038 ori_loss: 10.8038 l1_error: 0.000000\n",
      "ms_loss: 10.6986 ori_loss: 10.6986 l1_error: 0.000001\n",
      "ms_loss: 10.7311 ori_loss: 10.7311 l1_error: 0.000001\n",
      "ms_loss: 10.7240 ori_loss: 10.7240 l1_error: 0.000000\n",
      "ms_loss: 10.7738 ori_loss: 10.7738 l1_error: 0.000001\n",
      "ms_loss: 10.7387 ori_loss: 10.7387 l1_error: 0.000000\n",
      "ms_loss: 10.7396 ori_loss: 10.7396 l1_error: 0.000001\n",
      "ms_loss: 10.7303 ori_loss: 10.7303 l1_error: 0.000000\n",
      "ms_loss: 10.7615 ori_loss: 10.7615 l1_error: 0.000000\n",
      "ms_loss: 10.7606 ori_loss: 10.7606 l1_error: 0.000000\n",
      "ms_loss: 10.7599 ori_loss: 10.7599 l1_error: 0.000000\n",
      "ms_loss: 10.7133 ori_loss: 10.7133 l1_error: 0.000000\n",
      "ms_loss: 10.7031 ori_loss: 10.7031 l1_error: 0.000001\n",
      "ms_loss: 10.7363 ori_loss: 10.7363 l1_error: 0.000000\n",
      "ms_loss: 10.7631 ori_loss: 10.7631 l1_error: 0.000000\n",
      "ms_loss: 10.6815 ori_loss: 10.6815 l1_error: 0.000000\n",
      "ms_loss: 10.7936 ori_loss: 10.7936 l1_error: 0.000001\n",
      "ms_loss: 10.7913 ori_loss: 10.7913 l1_error: 0.000000\n",
      "ms_loss: 10.7088 ori_loss: 10.7088 l1_error: 0.000001\n",
      "ms_loss: 10.8178 ori_loss: 10.8178 l1_error: 0.000000\n",
      "ms_loss: 10.7542 ori_loss: 10.7542 l1_error: 0.000000\n",
      "ms_loss: 10.6703 ori_loss: 10.6703 l1_error: 0.000001\n",
      "ms_loss: 10.6246 ori_loss: 10.6246 l1_error: 0.000000\n",
      "ms_loss: 10.7567 ori_loss: 10.7567 l1_error: 0.000000\n",
      "ms_loss: 10.7793 ori_loss: 10.7793 l1_error: 0.000000\n",
      "ms_loss: 10.7369 ori_loss: 10.7369 l1_error: 0.000000\n",
      "ms_loss: 10.7703 ori_loss: 10.7703 l1_error: 0.000000\n",
      "ms_loss: 10.7523 ori_loss: 10.7523 l1_error: 0.000001\n",
      "ms_loss: 10.7097 ori_loss: 10.7097 l1_error: 0.000001\n",
      "ms_loss: 10.7125 ori_loss: 10.7125 l1_error: 0.000000\n",
      "ms_loss: 10.7614 ori_loss: 10.7614 l1_error: 0.000000\n",
      "ms_loss: 10.8115 ori_loss: 10.8115 l1_error: 0.000001\n",
      "ms_loss: 10.7052 ori_loss: 10.7052 l1_error: 0.000000\n",
      "ms_loss: 10.7320 ori_loss: 10.7320 l1_error: 0.000002\n",
      "ms_loss: 10.7102 ori_loss: 10.7102 l1_error: 0.000001\n",
      "ms_loss: 10.6248 ori_loss: 10.6248 l1_error: 0.000000\n",
      "ms_loss: 10.7155 ori_loss: 10.7155 l1_error: 0.000001\n",
      "ms_loss: 10.8035 ori_loss: 10.8035 l1_error: 0.000001\n",
      "ms_loss: 10.7441 ori_loss: 10.7441 l1_error: 0.000000\n",
      "ms_loss: 10.7019 ori_loss: 10.7019 l1_error: 0.000001\n",
      "ms_loss: 10.7131 ori_loss: 10.7131 l1_error: 0.000000\n",
      "ms_loss: 10.7471 ori_loss: 10.7471 l1_error: 0.000000\n",
      "ms_loss: 10.6979 ori_loss: 10.6979 l1_error: 0.000000\n",
      "ms_loss: 10.7258 ori_loss: 10.7258 l1_error: 0.000000\n",
      "ms_loss: 10.7460 ori_loss: 10.7460 l1_error: 0.000000\n",
      "ms_loss: 10.8284 ori_loss: 10.8284 l1_error: 0.000000\n",
      "ms_loss: 10.7694 ori_loss: 10.7694 l1_error: 0.000000\n",
      "ms_loss: 10.7369 ori_loss: 10.7369 l1_error: 0.000002\n",
      "ms_loss: 10.8295 ori_loss: 10.8295 l1_error: 0.000000\n",
      "ms_loss: 10.7634 ori_loss: 10.7634 l1_error: 0.000000\n",
      "ms_loss: 10.7218 ori_loss: 10.7218 l1_error: 0.000001\n",
      "ms_loss: 10.7440 ori_loss: 10.7440 l1_error: 0.000000\n",
      "ms_loss: 10.6292 ori_loss: 10.6292 l1_error: 0.000001\n",
      "ms_loss: 10.6632 ori_loss: 10.6632 l1_error: 0.000001\n",
      "ms_loss: 10.7836 ori_loss: 10.7836 l1_error: 0.000000\n",
      "ms_loss: 10.7239 ori_loss: 10.7239 l1_error: 0.000000\n",
      "ms_loss: 10.7788 ori_loss: 10.7788 l1_error: 0.000000\n",
      "ms_loss: 10.7474 ori_loss: 10.7474 l1_error: 0.000000\n",
      "ms_loss: 10.6413 ori_loss: 10.6413 l1_error: 0.000001\n",
      "ms_loss: 10.6907 ori_loss: 10.6907 l1_error: 0.000000\n",
      "ms_loss: 10.7161 ori_loss: 10.7161 l1_error: 0.000000\n",
      "ms_loss: 10.7275 ori_loss: 10.7275 l1_error: 0.000001\n",
      "ms_loss: 10.7446 ori_loss: 10.7446 l1_error: 0.000000\n",
      "ms_loss: 10.7645 ori_loss: 10.7645 l1_error: 0.000001\n",
      "ms_loss: 10.7865 ori_loss: 10.7865 l1_error: 0.000001\n",
      "ms_loss: 10.7890 ori_loss: 10.7890 l1_error: 0.000000\n",
      "ms_loss: 10.6864 ori_loss: 10.6864 l1_error: 0.000000\n",
      "ms_loss: 10.7139 ori_loss: 10.7139 l1_error: 0.000000\n",
      "ms_loss: 10.7096 ori_loss: 10.7096 l1_error: 0.000000\n",
      "ms_loss: 10.7861 ori_loss: 10.7861 l1_error: 0.000000\n",
      "ms_loss: 10.7984 ori_loss: 10.7984 l1_error: 0.000002\n",
      "ms_loss: 10.7853 ori_loss: 10.7853 l1_error: 0.000000\n",
      "ms_loss: 10.6867 ori_loss: 10.6867 l1_error: 0.000000\n",
      "ms_loss: 10.7462 ori_loss: 10.7462 l1_error: 0.000000\n",
      "ms_loss: 10.7292 ori_loss: 10.7292 l1_error: 0.000001\n",
      "ms_loss: 10.7781 ori_loss: 10.7781 l1_error: 0.000000\n",
      "ms_loss: 10.7617 ori_loss: 10.7617 l1_error: 0.000000\n",
      "ms_loss: 10.6794 ori_loss: 10.6794 l1_error: 0.000001\n",
      "ms_loss: 10.7689 ori_loss: 10.7689 l1_error: 0.000000\n",
      "ms_loss: 10.7639 ori_loss: 10.7639 l1_error: 0.000001\n",
      "ms_loss: 10.7573 ori_loss: 10.7573 l1_error: 0.000000\n",
      "ms_loss: 10.7308 ori_loss: 10.7308 l1_error: 0.000000\n",
      "ms_loss: 10.8126 ori_loss: 10.8126 l1_error: 0.000000\n",
      "ms_loss: 10.6898 ori_loss: 10.6898 l1_error: 0.000000\n",
      "ms_loss: 10.7434 ori_loss: 10.7434 l1_error: 0.000000\n",
      "ms_loss: 10.8503 ori_loss: 10.8503 l1_error: 0.000000\n",
      "ms_loss: 10.6534 ori_loss: 10.6534 l1_error: 0.000000\n",
      "ms_loss: 10.8360 ori_loss: 10.8360 l1_error: 0.000000\n",
      "ms_loss: 10.8674 ori_loss: 10.8674 l1_error: 0.000000\n",
      "ms_loss: 10.7855 ori_loss: 10.7855 l1_error: 0.000000\n",
      "ms_loss: 10.8446 ori_loss: 10.8446 l1_error: 0.000000\n",
      "ms_loss: 10.7501 ori_loss: 10.7501 l1_error: 0.000000\n",
      "ms_loss: 10.7582 ori_loss: 10.7582 l1_error: 0.000000\n",
      "ms_loss: 10.8001 ori_loss: 10.8001 l1_error: 0.000000\n",
      "ms_loss: 10.7574 ori_loss: 10.7574 l1_error: 0.000000\n",
      "ms_loss: 10.8533 ori_loss: 10.8533 l1_error: 0.000001\n",
      "ms_loss: 10.7901 ori_loss: 10.7901 l1_error: 0.000000\n",
      "ms_loss: 10.6941 ori_loss: 10.6941 l1_error: 0.000000\n",
      "ms_loss: 10.7322 ori_loss: 10.7322 l1_error: 0.000000\n",
      "ms_loss: 10.6753 ori_loss: 10.6753 l1_error: 0.000000\n",
      "ms_loss: 10.7518 ori_loss: 10.7518 l1_error: 0.000000\n",
      "ms_loss: 10.7741 ori_loss: 10.7741 l1_error: 0.000001\n",
      "ms_loss: 10.7985 ori_loss: 10.7985 l1_error: 0.000000\n",
      "ms_loss: 10.6875 ori_loss: 10.6875 l1_error: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ms_loss: 10.7920 ori_loss: 10.7920 l1_error: 0.000000\n",
      "ms_loss: 10.7951 ori_loss: 10.7951 l1_error: 0.000000\n",
      "ms_loss: 10.8571 ori_loss: 10.8571 l1_error: 0.000000\n",
      "ms_loss: 10.7370 ori_loss: 10.7370 l1_error: 0.000000\n",
      "ms_loss: 10.8114 ori_loss: 10.8114 l1_error: 0.000000\n",
      "ms_loss: 10.7832 ori_loss: 10.7832 l1_error: 0.000000\n",
      "ms_loss: 10.7723 ori_loss: 10.7723 l1_error: 0.000001\n",
      "ms_loss: 10.7148 ori_loss: 10.7148 l1_error: 0.000000\n",
      "ms_loss: 10.7187 ori_loss: 10.7187 l1_error: 0.000000\n",
      "ms_loss: 10.7376 ori_loss: 10.7376 l1_error: 0.000002\n",
      "ms_loss: 10.7666 ori_loss: 10.7666 l1_error: 0.000000\n",
      "ms_loss: 10.7400 ori_loss: 10.7400 l1_error: 0.000000\n",
      "ms_loss: 10.7042 ori_loss: 10.7042 l1_error: 0.000000\n",
      "ms_loss: 10.7558 ori_loss: 10.7558 l1_error: 0.000000\n",
      "ms_loss: 10.7581 ori_loss: 10.7581 l1_error: 0.000000\n",
      "ms_loss: 10.7162 ori_loss: 10.7162 l1_error: 0.000000\n",
      "ms_loss: 10.6694 ori_loss: 10.6694 l1_error: 0.000000\n",
      "ms_loss: 10.8175 ori_loss: 10.8175 l1_error: 0.000000\n",
      "ms_loss: 10.7583 ori_loss: 10.7583 l1_error: 0.000000\n",
      "ms_loss: 10.7741 ori_loss: 10.7741 l1_error: 0.000000\n",
      "ms_loss: 10.7224 ori_loss: 10.7224 l1_error: 0.000000\n",
      "ms_loss: 10.7851 ori_loss: 10.7851 l1_error: 0.000000\n",
      "ms_loss: 10.6133 ori_loss: 10.6133 l1_error: 0.000000\n",
      "ms_loss: 10.8004 ori_loss: 10.8004 l1_error: 0.000000\n",
      "ms_loss: 10.7479 ori_loss: 10.7479 l1_error: 0.000001\n",
      "ms_loss: 10.8302 ori_loss: 10.8302 l1_error: 0.000000\n",
      "ms_loss: 10.7575 ori_loss: 10.7575 l1_error: 0.000001\n",
      "ms_loss: 10.6460 ori_loss: 10.6460 l1_error: 0.000000\n",
      "ms_loss: 10.7590 ori_loss: 10.7590 l1_error: 0.000001\n",
      "ms_loss: 10.7691 ori_loss: 10.7691 l1_error: 0.000001\n",
      "ms_loss: 10.7269 ori_loss: 10.7269 l1_error: 0.000000\n",
      "ms_loss: 10.8328 ori_loss: 10.8328 l1_error: 0.000000\n",
      "ms_loss: 10.7272 ori_loss: 10.7272 l1_error: 0.000000\n",
      "ms_loss: 10.7208 ori_loss: 10.7208 l1_error: 0.000001\n",
      "ms_loss: 10.7578 ori_loss: 10.7578 l1_error: 0.000000\n",
      "ms_loss: 10.6762 ori_loss: 10.6762 l1_error: 0.000000\n",
      "ms_loss: 10.7145 ori_loss: 10.7145 l1_error: 0.000000\n",
      "ms_loss: 10.7379 ori_loss: 10.7379 l1_error: 0.000001\n",
      "ms_loss: 10.7419 ori_loss: 10.7419 l1_error: 0.000000\n",
      "ms_loss: 10.7854 ori_loss: 10.7854 l1_error: 0.000001\n",
      "ms_loss: 10.6804 ori_loss: 10.6804 l1_error: 0.000001\n",
      "ms_loss: 10.7623 ori_loss: 10.7623 l1_error: 0.000000\n",
      "ms_loss: 10.7332 ori_loss: 10.7332 l1_error: 0.000000\n",
      "ms_loss: 10.7962 ori_loss: 10.7962 l1_error: 0.000000\n",
      "ms_loss: 10.6898 ori_loss: 10.6898 l1_error: 0.000000\n",
      "ms_loss: 10.7284 ori_loss: 10.7284 l1_error: 0.000000\n",
      "ms_loss: 10.8012 ori_loss: 10.8012 l1_error: 0.000001\n",
      "ms_loss: 10.7248 ori_loss: 10.7248 l1_error: 0.000000\n",
      "ms_loss: 10.6617 ori_loss: 10.6617 l1_error: 0.000000\n",
      "ms_loss: 10.7747 ori_loss: 10.7747 l1_error: 0.000000\n",
      "ms_loss: 10.7960 ori_loss: 10.7960 l1_error: 0.000000\n",
      "ms_loss: 10.7615 ori_loss: 10.7615 l1_error: 0.000001\n",
      "ms_loss: 10.6783 ori_loss: 10.6783 l1_error: 0.000000\n",
      "ms_loss: 10.8126 ori_loss: 10.8126 l1_error: 0.000000\n",
      "ms_loss: 10.7828 ori_loss: 10.7828 l1_error: 0.000000\n",
      "ms_loss: 10.7189 ori_loss: 10.7189 l1_error: 0.000001\n",
      "ms_loss: 10.7394 ori_loss: 10.7394 l1_error: 0.000000\n",
      "ms_loss: 10.7298 ori_loss: 10.7298 l1_error: 0.000000\n",
      "ms_loss: 10.7806 ori_loss: 10.7806 l1_error: 0.000000\n",
      "ms_loss: 10.7502 ori_loss: 10.7502 l1_error: 0.000000\n",
      "ms_loss: 10.7575 ori_loss: 10.7575 l1_error: 0.000000\n",
      "ms_loss: 10.7778 ori_loss: 10.7778 l1_error: 0.000000\n",
      "ms_loss: 10.8420 ori_loss: 10.8420 l1_error: 0.000000\n",
      "ms_loss: 10.7460 ori_loss: 10.7460 l1_error: 0.000000\n",
      "ms_loss: 10.7764 ori_loss: 10.7764 l1_error: 0.000000\n",
      "ms_loss: 10.7669 ori_loss: 10.7669 l1_error: 0.000000\n",
      "ms_loss: 10.7465 ori_loss: 10.7465 l1_error: 0.000000\n",
      "ms_loss: 10.7485 ori_loss: 10.7485 l1_error: 0.000000\n",
      "ms_loss: 10.7171 ori_loss: 10.7171 l1_error: 0.000000\n",
      "ms_loss: 10.6976 ori_loss: 10.6976 l1_error: 0.000001\n",
      "ms_loss: 10.7064 ori_loss: 10.7064 l1_error: 0.000000\n",
      "ms_loss: 10.7769 ori_loss: 10.7769 l1_error: 0.000000\n",
      "ms_loss: 10.7488 ori_loss: 10.7488 l1_error: 0.000002\n",
      "ms_loss: 10.7679 ori_loss: 10.7679 l1_error: 0.000000\n",
      "ms_loss: 10.7446 ori_loss: 10.7446 l1_error: 0.000000\n",
      "ms_loss: 10.6829 ori_loss: 10.6829 l1_error: 0.000001\n",
      "ms_loss: 10.6640 ori_loss: 10.6640 l1_error: 0.000000\n",
      "ms_loss: 10.7242 ori_loss: 10.7242 l1_error: 0.000000\n",
      "ms_loss: 10.6264 ori_loss: 10.6264 l1_error: 0.000000\n",
      "ms_loss: 10.7952 ori_loss: 10.7952 l1_error: 0.000000\n",
      "ms_loss: 10.7044 ori_loss: 10.7044 l1_error: 0.000000\n",
      "ms_loss: 10.7492 ori_loss: 10.7492 l1_error: 0.000001\n",
      "ms_loss: 10.7414 ori_loss: 10.7414 l1_error: 0.000000\n",
      "ms_loss: 10.7116 ori_loss: 10.7116 l1_error: 0.000000\n",
      "ms_loss: 10.7173 ori_loss: 10.7173 l1_error: 0.000000\n",
      "ms_loss: 10.8740 ori_loss: 10.8740 l1_error: 0.000000\n",
      "ms_loss: 10.6152 ori_loss: 10.6152 l1_error: 0.000001\n",
      "ms_loss: 10.7358 ori_loss: 10.7358 l1_error: 0.000000\n",
      "ms_loss: 10.7369 ori_loss: 10.7369 l1_error: 0.000000\n",
      "ms_loss: 10.7431 ori_loss: 10.7431 l1_error: 0.000001\n",
      "ms_loss: 10.7577 ori_loss: 10.7577 l1_error: 0.000000\n",
      "ms_loss: 10.8144 ori_loss: 10.8144 l1_error: 0.000000\n",
      "ms_loss: 10.6897 ori_loss: 10.6897 l1_error: 0.000001\n",
      "ms_loss: 10.7614 ori_loss: 10.7614 l1_error: 0.000000\n",
      "ms_loss: 10.7497 ori_loss: 10.7497 l1_error: 0.000002\n",
      "ms_loss: 10.7637 ori_loss: 10.7637 l1_error: 0.000000\n",
      "ms_loss: 10.7757 ori_loss: 10.7757 l1_error: 0.000001\n",
      "ms_loss: 10.7223 ori_loss: 10.7223 l1_error: 0.000001\n",
      "ms_loss: 10.7293 ori_loss: 10.7293 l1_error: 0.000000\n",
      "ms_loss: 10.6690 ori_loss: 10.6690 l1_error: 0.000000\n",
      "ms_loss: 10.7295 ori_loss: 10.7295 l1_error: 0.000000\n",
      "ms_loss: 10.8403 ori_loss: 10.8403 l1_error: 0.000000\n",
      "ms_loss: 10.7890 ori_loss: 10.7890 l1_error: 0.000001\n",
      "ms_loss: 10.7371 ori_loss: 10.7371 l1_error: 0.000000\n",
      "ms_loss: 10.7504 ori_loss: 10.7504 l1_error: 0.000000\n",
      "ms_loss: 10.7620 ori_loss: 10.7620 l1_error: 0.000000\n",
      "ms_loss: 10.8171 ori_loss: 10.8171 l1_error: 0.000000\n",
      "ms_loss: 10.7965 ori_loss: 10.7965 l1_error: 0.000000\n",
      "ms_loss: 10.8019 ori_loss: 10.8019 l1_error: 0.000000\n",
      "ms_loss: 10.7805 ori_loss: 10.7805 l1_error: 0.000000\n",
      "ms_loss: 10.6882 ori_loss: 10.6882 l1_error: 0.000000\n",
      "ms_loss: 10.7718 ori_loss: 10.7718 l1_error: 0.000001\n",
      "ms_loss: 10.6729 ori_loss: 10.6729 l1_error: 0.000000\n",
      "ms_loss: 10.8674 ori_loss: 10.8674 l1_error: 0.000001\n",
      "ms_loss: 10.7496 ori_loss: 10.7496 l1_error: 0.000000\n",
      "ms_loss: 10.7867 ori_loss: 10.7867 l1_error: 0.000001\n",
      "ms_loss: 10.8004 ori_loss: 10.8004 l1_error: 0.000000\n",
      "ms_loss: 10.7948 ori_loss: 10.7948 l1_error: 0.000000\n",
      "ms_loss: 10.7274 ori_loss: 10.7274 l1_error: 0.000001\n",
      "ms_loss: 10.7668 ori_loss: 10.7668 l1_error: 0.000001\n",
      "ms_loss: 10.7304 ori_loss: 10.7304 l1_error: 0.000000\n",
      "ms_loss: 10.7360 ori_loss: 10.7360 l1_error: 0.000000\n",
      "ms_loss: 10.7154 ori_loss: 10.7154 l1_error: 0.000001\n",
      "ms_loss: 10.7478 ori_loss: 10.7478 l1_error: 0.000001\n",
      "ms_loss: 10.6759 ori_loss: 10.6759 l1_error: 0.000000\n",
      "ms_loss: 10.7596 ori_loss: 10.7596 l1_error: 0.000000\n",
      "ms_loss: 10.8001 ori_loss: 10.8001 l1_error: 0.000000\n",
      "ms_loss: 10.6879 ori_loss: 10.6879 l1_error: 0.000000\n",
      "ms_loss: 10.7348 ori_loss: 10.7348 l1_error: 0.000000\n",
      "ms_loss: 10.7645 ori_loss: 10.7645 l1_error: 0.000000\n",
      "ms_loss: 10.7402 ori_loss: 10.7402 l1_error: 0.000000\n",
      "ms_loss: 10.8648 ori_loss: 10.8648 l1_error: 0.000002\n",
      "ms_loss: 10.8245 ori_loss: 10.8245 l1_error: 0.000000\n",
      "ms_loss: 10.7313 ori_loss: 10.7313 l1_error: 0.000001\n",
      "ms_loss: 10.7414 ori_loss: 10.7414 l1_error: 0.000001\n",
      "ms_loss: 10.7893 ori_loss: 10.7893 l1_error: 0.000001\n",
      "ms_loss: 10.7545 ori_loss: 10.7545 l1_error: 0.000000\n",
      "ms_loss: 10.7781 ori_loss: 10.7781 l1_error: 0.000000\n",
      "ms_loss: 10.6987 ori_loss: 10.6987 l1_error: 0.000000\n",
      "ms_loss: 10.7649 ori_loss: 10.7649 l1_error: 0.000000\n",
      "ms_loss: 10.8095 ori_loss: 10.8095 l1_error: 0.000000\n",
      "ms_loss: 10.8404 ori_loss: 10.8404 l1_error: 0.000001\n",
      "ms_loss: 10.7951 ori_loss: 10.7951 l1_error: 0.000000\n",
      "ms_loss: 10.8321 ori_loss: 10.8321 l1_error: 0.000000\n",
      "ms_loss: 10.6874 ori_loss: 10.6874 l1_error: 0.000000\n",
      "ms_loss: 10.7454 ori_loss: 10.7454 l1_error: 0.000000\n",
      "ms_loss: 10.7099 ori_loss: 10.7099 l1_error: 0.000001\n",
      "ms_loss: 10.7515 ori_loss: 10.7515 l1_error: 0.000000\n",
      "ms_loss: 10.7593 ori_loss: 10.7593 l1_error: 0.000000\n",
      "ms_loss: 10.7855 ori_loss: 10.7855 l1_error: 0.000000\n",
      "ms_loss: 10.6735 ori_loss: 10.6735 l1_error: 0.000000\n",
      "ms_loss: 10.8030 ori_loss: 10.8030 l1_error: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ms_loss: 10.7699 ori_loss: 10.7699 l1_error: 0.000000\n",
      "ms_loss: 10.7252 ori_loss: 10.7252 l1_error: 0.000000\n",
      "ms_loss: 10.6117 ori_loss: 10.6117 l1_error: 0.000000\n",
      "ms_loss: 10.8004 ori_loss: 10.8004 l1_error: 0.000000\n",
      "ms_loss: 10.7266 ori_loss: 10.7266 l1_error: 0.000000\n",
      "ms_loss: 10.7641 ori_loss: 10.7641 l1_error: 0.000001\n",
      "ms_loss: 10.6789 ori_loss: 10.6789 l1_error: 0.000000\n",
      "ms_loss: 10.6931 ori_loss: 10.6931 l1_error: 0.000000\n",
      "ms_loss: 10.7343 ori_loss: 10.7343 l1_error: 0.000000\n",
      "ms_loss: 10.7539 ori_loss: 10.7539 l1_error: 0.000000\n",
      "ms_loss: 10.7838 ori_loss: 10.7838 l1_error: 0.000000\n",
      "ms_loss: 10.7284 ori_loss: 10.7284 l1_error: 0.000000\n",
      "ms_loss: 10.7321 ori_loss: 10.7321 l1_error: 0.000000\n",
      "ms_loss: 10.6576 ori_loss: 10.6576 l1_error: 0.000000\n",
      "ms_loss: 10.7966 ori_loss: 10.7966 l1_error: 0.000000\n",
      "ms_loss: 10.7884 ori_loss: 10.7884 l1_error: 0.000000\n",
      "ms_loss: 10.7616 ori_loss: 10.7616 l1_error: 0.000000\n",
      "ms_loss: 10.7251 ori_loss: 10.7251 l1_error: 0.000000\n",
      "ms_loss: 10.7820 ori_loss: 10.7820 l1_error: 0.000000\n",
      "ms_loss: 10.7918 ori_loss: 10.7918 l1_error: 0.000000\n",
      "ms_loss: 10.7806 ori_loss: 10.7806 l1_error: 0.000000\n",
      "ms_loss: 10.7225 ori_loss: 10.7225 l1_error: 0.000000\n",
      "ms_loss: 10.7426 ori_loss: 10.7426 l1_error: 0.000000\n",
      "ms_loss: 10.6481 ori_loss: 10.6481 l1_error: 0.000000\n",
      "ms_loss: 10.7931 ori_loss: 10.7931 l1_error: 0.000000\n",
      "ms_loss: 10.7255 ori_loss: 10.7255 l1_error: 0.000001\n",
      "ms_loss: 10.8086 ori_loss: 10.8086 l1_error: 0.000001\n",
      "ms_loss: 10.7737 ori_loss: 10.7737 l1_error: 0.000000\n",
      "ms_loss: 10.7350 ori_loss: 10.7350 l1_error: 0.000000\n",
      "ms_loss: 10.7901 ori_loss: 10.7901 l1_error: 0.000000\n",
      "ms_loss: 10.7528 ori_loss: 10.7528 l1_error: 0.000000\n",
      "ms_loss: 10.7611 ori_loss: 10.7611 l1_error: 0.000001\n",
      "ms_loss: 10.7094 ori_loss: 10.7094 l1_error: 0.000000\n",
      "ms_loss: 10.7419 ori_loss: 10.7419 l1_error: 0.000001\n",
      "ms_loss: 10.6714 ori_loss: 10.6714 l1_error: 0.000000\n",
      "ms_loss: 10.6758 ori_loss: 10.6758 l1_error: 0.000000\n",
      "ms_loss: 10.7338 ori_loss: 10.7338 l1_error: 0.000001\n",
      "ms_loss: 10.7281 ori_loss: 10.7280 l1_error: 0.000001\n",
      "ms_loss: 10.7978 ori_loss: 10.7978 l1_error: 0.000000\n",
      "ms_loss: 10.7288 ori_loss: 10.7288 l1_error: 0.000000\n",
      "ms_loss: 10.7510 ori_loss: 10.7510 l1_error: 0.000001\n",
      "ms_loss: 10.7418 ori_loss: 10.7418 l1_error: 0.000000\n",
      "ms_loss: 10.7250 ori_loss: 10.7250 l1_error: 0.000001\n",
      "ms_loss: 10.6803 ori_loss: 10.6803 l1_error: 0.000000\n",
      "ms_loss: 10.7280 ori_loss: 10.7280 l1_error: 0.000000\n",
      "ms_loss: 10.7387 ori_loss: 10.7387 l1_error: 0.000000\n",
      "ms_loss: 10.7213 ori_loss: 10.7213 l1_error: 0.000001\n",
      "ms_loss: 10.8421 ori_loss: 10.8421 l1_error: 0.000000\n",
      "ms_loss: 10.7566 ori_loss: 10.7566 l1_error: 0.000000\n",
      "ms_loss: 10.8475 ori_loss: 10.8475 l1_error: 0.000000\n",
      "ms_loss: 10.8617 ori_loss: 10.8617 l1_error: 0.000000\n",
      "ms_loss: 10.6908 ori_loss: 10.6908 l1_error: 0.000000\n",
      "ms_loss: 10.7187 ori_loss: 10.7187 l1_error: 0.000000\n",
      "ms_loss: 10.7207 ori_loss: 10.7207 l1_error: 0.000000\n",
      "ms_loss: 10.6823 ori_loss: 10.6823 l1_error: 0.000001\n",
      "ms_loss: 10.7536 ori_loss: 10.7536 l1_error: 0.000000\n",
      "ms_loss: 10.8570 ori_loss: 10.8570 l1_error: 0.000001\n",
      "ms_loss: 10.7228 ori_loss: 10.7228 l1_error: 0.000000\n",
      "ms_loss: 10.7132 ori_loss: 10.7132 l1_error: 0.000000\n",
      "ms_loss: 10.6765 ori_loss: 10.6765 l1_error: 0.000001\n",
      "ms_loss: 10.8248 ori_loss: 10.8248 l1_error: 0.000002\n",
      "ms_loss: 10.6998 ori_loss: 10.6998 l1_error: 0.000000\n",
      "ms_loss: 10.7768 ori_loss: 10.7768 l1_error: 0.000000\n",
      "ms_loss: 10.8164 ori_loss: 10.8164 l1_error: 0.000001\n",
      "ms_loss: 10.7406 ori_loss: 10.7406 l1_error: 0.000000\n",
      "ms_loss: 10.6741 ori_loss: 10.6741 l1_error: 0.000000\n",
      "ms_loss: 10.7023 ori_loss: 10.7023 l1_error: 0.000000\n",
      "ms_loss: 10.6950 ori_loss: 10.6950 l1_error: 0.000001\n",
      "ms_loss: 10.7123 ori_loss: 10.7123 l1_error: 0.000000\n",
      "ms_loss: 10.6924 ori_loss: 10.6924 l1_error: 0.000000\n",
      "ms_loss: 10.7443 ori_loss: 10.7443 l1_error: 0.000000\n",
      "ms_loss: 10.7285 ori_loss: 10.7285 l1_error: 0.000000\n",
      "ms_loss: 10.6823 ori_loss: 10.6823 l1_error: 0.000000\n",
      "ms_loss: 10.7462 ori_loss: 10.7462 l1_error: 0.000000\n",
      "ms_loss: 10.8163 ori_loss: 10.8163 l1_error: 0.000000\n",
      "ms_loss: 10.7638 ori_loss: 10.7638 l1_error: 0.000000\n",
      "ms_loss: 10.6734 ori_loss: 10.6734 l1_error: 0.000001\n",
      "ms_loss: 10.8094 ori_loss: 10.8094 l1_error: 0.000000\n",
      "ms_loss: 10.6696 ori_loss: 10.6696 l1_error: 0.000000\n",
      "ms_loss: 10.7449 ori_loss: 10.7449 l1_error: 0.000000\n",
      "ms_loss: 10.7730 ori_loss: 10.7730 l1_error: 0.000000\n",
      "ms_loss: 10.8078 ori_loss: 10.8078 l1_error: 0.000000\n",
      "ms_loss: 10.7058 ori_loss: 10.7058 l1_error: 0.000001\n",
      "ms_loss: 10.7361 ori_loss: 10.7361 l1_error: 0.000001\n",
      "ms_loss: 10.7480 ori_loss: 10.7480 l1_error: 0.000000\n",
      "ms_loss: 10.8031 ori_loss: 10.8031 l1_error: 0.000000\n",
      "ms_loss: 10.7708 ori_loss: 10.7708 l1_error: 0.000001\n",
      "ms_loss: 10.7080 ori_loss: 10.7080 l1_error: 0.000000\n",
      "ms_loss: 10.7439 ori_loss: 10.7439 l1_error: 0.000000\n",
      "ms_loss: 10.8038 ori_loss: 10.8038 l1_error: 0.000000\n",
      "ms_loss: 10.6490 ori_loss: 10.6490 l1_error: 0.000000\n",
      "ms_loss: 10.7637 ori_loss: 10.7637 l1_error: 0.000001\n",
      "ms_loss: 10.7338 ori_loss: 10.7338 l1_error: 0.000000\n",
      "ms_loss: 10.7249 ori_loss: 10.7249 l1_error: 0.000001\n",
      "ms_loss: 10.7922 ori_loss: 10.7922 l1_error: 0.000002\n",
      "ms_loss: 10.6746 ori_loss: 10.6746 l1_error: 0.000000\n",
      "ms_loss: 10.7616 ori_loss: 10.7616 l1_error: 0.000000\n",
      "ms_loss: 10.8510 ori_loss: 10.8510 l1_error: 0.000000\n",
      "ms_loss: 10.7901 ori_loss: 10.7901 l1_error: 0.000001\n",
      "ms_loss: 10.8157 ori_loss: 10.8157 l1_error: 0.000000\n",
      "ms_loss: 10.7407 ori_loss: 10.7407 l1_error: 0.000001\n",
      "ms_loss: 10.7713 ori_loss: 10.7713 l1_error: 0.000000\n",
      "ms_loss: 10.8697 ori_loss: 10.8697 l1_error: 0.000000\n",
      "ms_loss: 10.7789 ori_loss: 10.7789 l1_error: 0.000000\n",
      "ms_loss: 10.8024 ori_loss: 10.8024 l1_error: 0.000000\n",
      "ms_loss: 10.6849 ori_loss: 10.6849 l1_error: 0.000001\n",
      "ms_loss: 10.7999 ori_loss: 10.7999 l1_error: 0.000002\n",
      "ms_loss: 10.7731 ori_loss: 10.7731 l1_error: 0.000000\n",
      "ms_loss: 10.7123 ori_loss: 10.7123 l1_error: 0.000002\n",
      "ms_loss: 10.8018 ori_loss: 10.8018 l1_error: 0.000001\n",
      "ms_loss: 10.6953 ori_loss: 10.6953 l1_error: 0.000000\n",
      "ms_loss: 10.8390 ori_loss: 10.8390 l1_error: 0.000000\n",
      "ms_loss: 10.7518 ori_loss: 10.7518 l1_error: 0.000000\n",
      "ms_loss: 10.7741 ori_loss: 10.7741 l1_error: 0.000000\n",
      "ms_loss: 10.7380 ori_loss: 10.7380 l1_error: 0.000000\n",
      "ms_loss: 10.7997 ori_loss: 10.7997 l1_error: 0.000000\n",
      "ms_loss: 10.7328 ori_loss: 10.7328 l1_error: 0.000000\n",
      "ms_loss: 10.7552 ori_loss: 10.7552 l1_error: 0.000002\n",
      "ms_loss: 10.7580 ori_loss: 10.7580 l1_error: 0.000000\n",
      "ms_loss: 10.7376 ori_loss: 10.7376 l1_error: 0.000001\n",
      "ms_loss: 10.6949 ori_loss: 10.6949 l1_error: 0.000000\n",
      "ms_loss: 10.5973 ori_loss: 10.5973 l1_error: 0.000001\n",
      "ms_loss: 10.7632 ori_loss: 10.7632 l1_error: 0.000000\n",
      "ms_loss: 10.8030 ori_loss: 10.8030 l1_error: 0.000000\n",
      "ms_loss: 10.7783 ori_loss: 10.7783 l1_error: 0.000001\n",
      "ms_loss: 10.7198 ori_loss: 10.7198 l1_error: 0.000000\n",
      "ms_loss: 10.6903 ori_loss: 10.6903 l1_error: 0.000000\n",
      "ms_loss: 10.6618 ori_loss: 10.6618 l1_error: 0.000000\n",
      "ms_loss: 10.7019 ori_loss: 10.7019 l1_error: 0.000000\n",
      "ms_loss: 10.7598 ori_loss: 10.7598 l1_error: 0.000001\n",
      "ms_loss: 10.7279 ori_loss: 10.7279 l1_error: 0.000000\n",
      "ms_loss: 10.8161 ori_loss: 10.8161 l1_error: 0.000000\n",
      "ms_loss: 10.7845 ori_loss: 10.7845 l1_error: 0.000001\n",
      "ms_loss: 10.7536 ori_loss: 10.7536 l1_error: 0.000000\n",
      "ms_loss: 10.6476 ori_loss: 10.6476 l1_error: 0.000000\n",
      "ms_loss: 10.7268 ori_loss: 10.7268 l1_error: 0.000001\n",
      "ms_loss: 10.6779 ori_loss: 10.6779 l1_error: 0.000000\n",
      "ms_loss: 10.6176 ori_loss: 10.6176 l1_error: 0.000000\n",
      "ms_loss: 10.7240 ori_loss: 10.7240 l1_error: 0.000000\n",
      "ms_loss: 10.7833 ori_loss: 10.7833 l1_error: 0.000000\n",
      "ms_loss: 10.7728 ori_loss: 10.7728 l1_error: 0.000000\n",
      "ms_loss: 10.7750 ori_loss: 10.7750 l1_error: 0.000000\n",
      "ms_loss: 10.7268 ori_loss: 10.7268 l1_error: 0.000001\n",
      "ms_loss: 10.7083 ori_loss: 10.7083 l1_error: 0.000000\n",
      "ms_loss: 10.7482 ori_loss: 10.7482 l1_error: 0.000000\n",
      "ms_loss: 10.7595 ori_loss: 10.7595 l1_error: 0.000000\n",
      "ms_loss: 10.7576 ori_loss: 10.7576 l1_error: 0.000000\n",
      "ms_loss: 10.7092 ori_loss: 10.7092 l1_error: 0.000000\n",
      "ms_loss: 10.7616 ori_loss: 10.7616 l1_error: 0.000000\n",
      "ms_loss: 10.7076 ori_loss: 10.7076 l1_error: 0.000001\n",
      "ms_loss: 10.6952 ori_loss: 10.6952 l1_error: 0.000000\n",
      "ms_loss: 10.7706 ori_loss: 10.7706 l1_error: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ms_loss: 10.7820 ori_loss: 10.7820 l1_error: 0.000000\n",
      "ms_loss: 10.6864 ori_loss: 10.6864 l1_error: 0.000000\n",
      "ms_loss: 10.7614 ori_loss: 10.7614 l1_error: 0.000000\n",
      "ms_loss: 10.7166 ori_loss: 10.7166 l1_error: 0.000000\n",
      "ms_loss: 10.7781 ori_loss: 10.7781 l1_error: 0.000000\n",
      "ms_loss: 10.7970 ori_loss: 10.7970 l1_error: 0.000000\n",
      "ms_loss: 10.7694 ori_loss: 10.7694 l1_error: 0.000000\n",
      "ms_loss: 10.7701 ori_loss: 10.7701 l1_error: 0.000000\n",
      "ms_loss: 10.7303 ori_loss: 10.7303 l1_error: 0.000000\n",
      "ms_loss: 10.7937 ori_loss: 10.7937 l1_error: 0.000000\n",
      "ms_loss: 10.8167 ori_loss: 10.8167 l1_error: 0.000001\n",
      "ms_loss: 10.7595 ori_loss: 10.7595 l1_error: 0.000000\n",
      "ms_loss: 10.7964 ori_loss: 10.7964 l1_error: 0.000001\n",
      "ms_loss: 10.7279 ori_loss: 10.7279 l1_error: 0.000000\n",
      "ms_loss: 10.6673 ori_loss: 10.6673 l1_error: 0.000000\n",
      "ms_loss: 10.7662 ori_loss: 10.7662 l1_error: 0.000000\n",
      "ms_loss: 10.7186 ori_loss: 10.7186 l1_error: 0.000000\n",
      "ms_loss: 10.7410 ori_loss: 10.7410 l1_error: 0.000002\n",
      "ms_loss: 10.8144 ori_loss: 10.8144 l1_error: 0.000000\n",
      "ms_loss: 10.7725 ori_loss: 10.7725 l1_error: 0.000001\n",
      "ms_loss: 10.7683 ori_loss: 10.7683 l1_error: 0.000000\n",
      "ms_loss: 10.7380 ori_loss: 10.7380 l1_error: 0.000000\n",
      "ms_loss: 10.7643 ori_loss: 10.7643 l1_error: 0.000000\n",
      "ms_loss: 10.7046 ori_loss: 10.7046 l1_error: 0.000000\n",
      "ms_loss: 10.7897 ori_loss: 10.7897 l1_error: 0.000000\n",
      "ms_loss: 10.6902 ori_loss: 10.6902 l1_error: 0.000000\n",
      "ms_loss: 10.6898 ori_loss: 10.6898 l1_error: 0.000000\n",
      "ms_loss: 10.7701 ori_loss: 10.7701 l1_error: 0.000001\n",
      "ms_loss: 10.8561 ori_loss: 10.8561 l1_error: 0.000000\n",
      "ms_loss: 10.7215 ori_loss: 10.7215 l1_error: 0.000001\n",
      "ms_loss: 10.7284 ori_loss: 10.7284 l1_error: 0.000001\n",
      "ms_loss: 10.7820 ori_loss: 10.7820 l1_error: 0.000000\n",
      "ms_loss: 10.7415 ori_loss: 10.7415 l1_error: 0.000001\n",
      "ms_loss: 10.7096 ori_loss: 10.7096 l1_error: 0.000001\n",
      "ms_loss: 10.6889 ori_loss: 10.6889 l1_error: 0.000000\n",
      "ms_loss: 10.7552 ori_loss: 10.7552 l1_error: 0.000001\n",
      "ms_loss: 10.7369 ori_loss: 10.7369 l1_error: 0.000000\n",
      "ms_loss: 10.6911 ori_loss: 10.6911 l1_error: 0.000000\n",
      "ms_loss: 10.7043 ori_loss: 10.7043 l1_error: 0.000000\n",
      "ms_loss: 10.7020 ori_loss: 10.7020 l1_error: 0.000000\n",
      "ms_loss: 10.7493 ori_loss: 10.7493 l1_error: 0.000000\n",
      "ms_loss: 10.8156 ori_loss: 10.8156 l1_error: 0.000000\n",
      "ms_loss: 10.7650 ori_loss: 10.7650 l1_error: 0.000001\n",
      "ms_loss: 10.7581 ori_loss: 10.7581 l1_error: 0.000001\n",
      "ms_loss: 10.7793 ori_loss: 10.7793 l1_error: 0.000000\n",
      "ms_loss: 10.7169 ori_loss: 10.7169 l1_error: 0.000000\n",
      "ms_loss: 10.6965 ori_loss: 10.6965 l1_error: 0.000000\n",
      "ms_loss: 10.7430 ori_loss: 10.7430 l1_error: 0.000000\n",
      "ms_loss: 10.7453 ori_loss: 10.7453 l1_error: 0.000000\n",
      "ms_loss: 10.6906 ori_loss: 10.6906 l1_error: 0.000000\n",
      "ms_loss: 10.7728 ori_loss: 10.7728 l1_error: 0.000000\n",
      "ms_loss: 10.6440 ori_loss: 10.6440 l1_error: 0.000000\n",
      "ms_loss: 10.7182 ori_loss: 10.7182 l1_error: 0.000000\n",
      "ms_loss: 10.6467 ori_loss: 10.6467 l1_error: 0.000000\n",
      "ms_loss: 10.8364 ori_loss: 10.8364 l1_error: 0.000000\n",
      "ms_loss: 10.7316 ori_loss: 10.7316 l1_error: 0.000000\n",
      "ms_loss: 10.7783 ori_loss: 10.7783 l1_error: 0.000000\n",
      "ms_loss: 10.7830 ori_loss: 10.7830 l1_error: 0.000000\n",
      "ms_loss: 10.6842 ori_loss: 10.6842 l1_error: 0.000001\n",
      "ms_loss: 10.7510 ori_loss: 10.7510 l1_error: 0.000001\n",
      "ms_loss: 10.6501 ori_loss: 10.6501 l1_error: 0.000000\n",
      "ms_loss: 10.7261 ori_loss: 10.7261 l1_error: 0.000001\n",
      "ms_loss: 10.8448 ori_loss: 10.8448 l1_error: 0.000001\n",
      "ms_loss: 10.6358 ori_loss: 10.6358 l1_error: 0.000000\n",
      "ms_loss: 10.7447 ori_loss: 10.7447 l1_error: 0.000000\n",
      "ms_loss: 10.8352 ori_loss: 10.8352 l1_error: 0.000000\n",
      "ms_loss: 10.7610 ori_loss: 10.7610 l1_error: 0.000000\n",
      "ms_loss: 10.7861 ori_loss: 10.7861 l1_error: 0.000000\n",
      "ms_loss: 10.7709 ori_loss: 10.7709 l1_error: 0.000000\n",
      "ms_loss: 10.6445 ori_loss: 10.6445 l1_error: 0.000000\n",
      "ms_loss: 10.7409 ori_loss: 10.7409 l1_error: 0.000000\n",
      "ms_loss: 10.7740 ori_loss: 10.7740 l1_error: 0.000002\n",
      "ms_loss: 10.7531 ori_loss: 10.7531 l1_error: 0.000001\n",
      "ms_loss: 10.7388 ori_loss: 10.7388 l1_error: 0.000000\n",
      "ms_loss: 10.7307 ori_loss: 10.7307 l1_error: 0.000000\n",
      "ms_loss: 10.8426 ori_loss: 10.8426 l1_error: 0.000000\n",
      "ms_loss: 10.7356 ori_loss: 10.7356 l1_error: 0.000001\n",
      "ms_loss: 10.8314 ori_loss: 10.8314 l1_error: 0.000001\n",
      "ms_loss: 10.7476 ori_loss: 10.7476 l1_error: 0.000001\n",
      "ms_loss: 10.6853 ori_loss: 10.6853 l1_error: 0.000000\n",
      "ms_loss: 10.8006 ori_loss: 10.8006 l1_error: 0.000000\n",
      "ms_loss: 10.7731 ori_loss: 10.7731 l1_error: 0.000001\n",
      "ms_loss: 10.7470 ori_loss: 10.7470 l1_error: 0.000001\n",
      "ms_loss: 10.7847 ori_loss: 10.7847 l1_error: 0.000000\n",
      "ms_loss: 10.8597 ori_loss: 10.8597 l1_error: 0.000000\n",
      "ms_loss: 10.7291 ori_loss: 10.7291 l1_error: 0.000000\n",
      "ms_loss: 10.7964 ori_loss: 10.7964 l1_error: 0.000000\n",
      "ms_loss: 10.7069 ori_loss: 10.7069 l1_error: 0.000000\n",
      "ms_loss: 10.7146 ori_loss: 10.7146 l1_error: 0.000001\n",
      "ms_loss: 10.6843 ori_loss: 10.6843 l1_error: 0.000000\n",
      "ms_loss: 10.7846 ori_loss: 10.7846 l1_error: 0.000000\n",
      "ms_loss: 10.8119 ori_loss: 10.8119 l1_error: 0.000001\n",
      "ms_loss: 10.7943 ori_loss: 10.7943 l1_error: 0.000000\n",
      "ms_loss: 10.7536 ori_loss: 10.7536 l1_error: 0.000000\n",
      "ms_loss: 10.7146 ori_loss: 10.7146 l1_error: 0.000000\n",
      "ms_loss: 10.7604 ori_loss: 10.7604 l1_error: 0.000001\n",
      "ms_loss: 10.7272 ori_loss: 10.7272 l1_error: 0.000001\n",
      "ms_loss: 10.7029 ori_loss: 10.7029 l1_error: 0.000000\n",
      "ms_loss: 10.7198 ori_loss: 10.7198 l1_error: 0.000001\n",
      "ms_loss: 10.7880 ori_loss: 10.7880 l1_error: 0.000000\n",
      "ms_loss: 10.8107 ori_loss: 10.8107 l1_error: 0.000000\n",
      "ms_loss: 10.7283 ori_loss: 10.7283 l1_error: 0.000000\n",
      "ms_loss: 10.7714 ori_loss: 10.7714 l1_error: 0.000001\n",
      "ms_loss: 10.7267 ori_loss: 10.7267 l1_error: 0.000000\n",
      "ms_loss: 10.7268 ori_loss: 10.7268 l1_error: 0.000000\n",
      "ms_loss: 10.6663 ori_loss: 10.6663 l1_error: 0.000000\n",
      "ms_loss: 10.8269 ori_loss: 10.8269 l1_error: 0.000000\n",
      "ms_loss: 10.7154 ori_loss: 10.7154 l1_error: 0.000001\n",
      "ms_loss: 10.7531 ori_loss: 10.7531 l1_error: 0.000000\n",
      "ms_loss: 10.7171 ori_loss: 10.7171 l1_error: 0.000000\n",
      "ms_loss: 10.7325 ori_loss: 10.7325 l1_error: 0.000000\n",
      "ms_loss: 10.8138 ori_loss: 10.8138 l1_error: 0.000001\n",
      "ms_loss: 10.8126 ori_loss: 10.8126 l1_error: 0.000000\n",
      "ms_loss: 10.7292 ori_loss: 10.7292 l1_error: 0.000000\n",
      "ms_loss: 10.8009 ori_loss: 10.8009 l1_error: 0.000000\n",
      "ms_loss: 10.7877 ori_loss: 10.7877 l1_error: 0.000000\n",
      "ms_loss: 10.7239 ori_loss: 10.7239 l1_error: 0.000000\n",
      "ms_loss: 10.7818 ori_loss: 10.7818 l1_error: 0.000000\n",
      "ms_loss: 10.7384 ori_loss: 10.7384 l1_error: 0.000000\n",
      "ms_loss: 10.8189 ori_loss: 10.8189 l1_error: 0.000000\n",
      "ms_loss: 10.7303 ori_loss: 10.7303 l1_error: 0.000000\n",
      "ms_loss: 10.7626 ori_loss: 10.7626 l1_error: 0.000001\n",
      "ms_loss: 10.6982 ori_loss: 10.6982 l1_error: 0.000000\n",
      "ms_loss: 10.7799 ori_loss: 10.7799 l1_error: 0.000000\n",
      "ms_loss: 10.8005 ori_loss: 10.8005 l1_error: 0.000000\n",
      "ms_loss: 10.8097 ori_loss: 10.8097 l1_error: 0.000000\n",
      "ms_loss: 10.7589 ori_loss: 10.7589 l1_error: 0.000000\n",
      "ms_loss: 10.8203 ori_loss: 10.8203 l1_error: 0.000000\n",
      "ms_loss: 10.7772 ori_loss: 10.7772 l1_error: 0.000001\n",
      "ms_loss: 10.7374 ori_loss: 10.7374 l1_error: 0.000000\n",
      "ms_loss: 10.7314 ori_loss: 10.7314 l1_error: 0.000000\n",
      "ms_loss: 10.8539 ori_loss: 10.8539 l1_error: 0.000000\n",
      "ms_loss: 10.7841 ori_loss: 10.7841 l1_error: 0.000000\n",
      "ms_loss: 10.7434 ori_loss: 10.7434 l1_error: 0.000002\n",
      "ms_loss: 10.7554 ori_loss: 10.7554 l1_error: 0.000001\n",
      "ms_loss: 10.7361 ori_loss: 10.7361 l1_error: 0.000000\n",
      "ms_loss: 10.8050 ori_loss: 10.8050 l1_error: 0.000000\n",
      "ms_loss: 10.7754 ori_loss: 10.7754 l1_error: 0.000001\n",
      "ms_loss: 10.7693 ori_loss: 10.7693 l1_error: 0.000001\n",
      "ms_loss: 10.7584 ori_loss: 10.7584 l1_error: 0.000000\n",
      "ms_loss: 10.7969 ori_loss: 10.7969 l1_error: 0.000000\n",
      "ms_loss: 10.8320 ori_loss: 10.8320 l1_error: 0.000002\n",
      "ms_loss: 10.8258 ori_loss: 10.8258 l1_error: 0.000000\n",
      "ms_loss: 10.7246 ori_loss: 10.7246 l1_error: 0.000000\n",
      "ms_loss: 10.7175 ori_loss: 10.7175 l1_error: 0.000001\n",
      "ms_loss: 10.7522 ori_loss: 10.7522 l1_error: 0.000001\n",
      "ms_loss: 10.7182 ori_loss: 10.7182 l1_error: 0.000000\n",
      "ms_loss: 10.7884 ori_loss: 10.7884 l1_error: 0.000000\n",
      "ms_loss: 10.6982 ori_loss: 10.6982 l1_error: 0.000000\n",
      "ms_loss: 10.6936 ori_loss: 10.6936 l1_error: 0.000001\n",
      "ms_loss: 10.7477 ori_loss: 10.7477 l1_error: 0.000001\n",
      "ms_loss: 10.8152 ori_loss: 10.8152 l1_error: 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ms_loss: 10.6749 ori_loss: 10.6749 l1_error: 0.000001\n",
      "ms_loss: 10.7678 ori_loss: 10.7678 l1_error: 0.000000\n",
      "ms_loss: 10.7231 ori_loss: 10.7231 l1_error: 0.000000\n",
      "ms_loss: 10.7549 ori_loss: 10.7549 l1_error: 0.000000\n",
      "ms_loss: 10.6931 ori_loss: 10.6931 l1_error: 0.000001\n",
      "ms_loss: 10.7437 ori_loss: 10.7437 l1_error: 0.000000\n",
      "ms_loss: 10.7163 ori_loss: 10.7163 l1_error: 0.000001\n",
      "ms_loss: 10.7545 ori_loss: 10.7545 l1_error: 0.000000\n",
      "ms_loss: 10.7260 ori_loss: 10.7260 l1_error: 0.000000\n",
      "ms_loss: 10.7669 ori_loss: 10.7669 l1_error: 0.000000\n",
      "ms_loss: 10.6364 ori_loss: 10.6364 l1_error: 0.000001\n",
      "ms_loss: 10.6867 ori_loss: 10.6867 l1_error: 0.000000\n",
      "ms_loss: 10.7168 ori_loss: 10.7168 l1_error: 0.000000\n",
      "ms_loss: 10.8642 ori_loss: 10.8642 l1_error: 0.000001\n",
      "ms_loss: 10.7370 ori_loss: 10.7370 l1_error: 0.000000\n",
      "ms_loss: 10.6926 ori_loss: 10.6926 l1_error: 0.000000\n",
      "ms_loss: 10.7416 ori_loss: 10.7416 l1_error: 0.000000\n",
      "ms_loss: 10.7700 ori_loss: 10.7700 l1_error: 0.000001\n",
      "ms_loss: 10.7570 ori_loss: 10.7570 l1_error: 0.000000\n",
      "ms_loss: 10.7869 ori_loss: 10.7869 l1_error: 0.000000\n",
      "ms_loss: 10.7310 ori_loss: 10.7310 l1_error: 0.000000\n",
      "ms_loss: 10.7101 ori_loss: 10.7101 l1_error: 0.000000\n",
      "ms_loss: 10.8185 ori_loss: 10.8185 l1_error: 0.000000\n",
      "ms_loss: 10.7916 ori_loss: 10.7916 l1_error: 0.000000\n",
      "ms_loss: 10.6902 ori_loss: 10.6902 l1_error: 0.000000\n",
      "ms_loss: 10.7909 ori_loss: 10.7909 l1_error: 0.000000\n",
      "ms_loss: 10.7674 ori_loss: 10.7674 l1_error: 0.000000\n",
      "ms_loss: 10.7550 ori_loss: 10.7550 l1_error: 0.000000\n",
      "ms_loss: 10.7513 ori_loss: 10.7513 l1_error: 0.000000\n",
      "ms_loss: 10.6762 ori_loss: 10.6762 l1_error: 0.000000\n",
      "ms_loss: 10.7856 ori_loss: 10.7856 l1_error: 0.000000\n",
      "ms_loss: 10.6847 ori_loss: 10.6847 l1_error: 0.000000\n",
      "ms_loss: 10.7383 ori_loss: 10.7383 l1_error: 0.000000\n",
      "ms_loss: 10.6803 ori_loss: 10.6803 l1_error: 0.000000\n",
      "ms_loss: 10.7307 ori_loss: 10.7307 l1_error: 0.000000\n",
      "ms_loss: 10.7629 ori_loss: 10.7629 l1_error: 0.000001\n",
      "ms_loss: 10.8016 ori_loss: 10.8016 l1_error: 0.000000\n",
      "ms_loss: 10.8072 ori_loss: 10.8072 l1_error: 0.000000\n",
      "ms_loss: 10.7606 ori_loss: 10.7606 l1_error: 0.000000\n",
      "ms_loss: 10.7011 ori_loss: 10.7011 l1_error: 0.000000\n",
      "ms_loss: 10.7475 ori_loss: 10.7475 l1_error: 0.000000\n",
      "ms_loss: 10.7001 ori_loss: 10.7001 l1_error: 0.000000\n",
      "ms_loss: 10.7635 ori_loss: 10.7635 l1_error: 0.000000\n",
      "ms_loss: 10.7241 ori_loss: 10.7241 l1_error: 0.000001\n",
      "ms_loss: 10.7363 ori_loss: 10.7363 l1_error: 0.000000\n",
      "ms_loss: 10.8653 ori_loss: 10.8653 l1_error: 0.000000\n",
      "ms_loss: 10.8788 ori_loss: 10.8788 l1_error: 0.000000\n",
      "ms_loss: 10.7662 ori_loss: 10.7662 l1_error: 0.000000\n",
      "ms_loss: 10.7326 ori_loss: 10.7326 l1_error: 0.000001\n",
      "ms_loss: 10.8372 ori_loss: 10.8372 l1_error: 0.000001\n",
      "ms_loss: 10.6912 ori_loss: 10.6912 l1_error: 0.000000\n",
      "ms_loss: 10.7550 ori_loss: 10.7550 l1_error: 0.000001\n",
      "ms_loss: 10.7606 ori_loss: 10.7606 l1_error: 0.000000\n",
      "ms_loss: 10.8330 ori_loss: 10.8330 l1_error: 0.000000\n",
      "ms_loss: 10.7408 ori_loss: 10.7408 l1_error: 0.000001\n",
      "ms_loss: 10.6852 ori_loss: 10.6852 l1_error: 0.000001\n",
      "ms_loss: 10.7038 ori_loss: 10.7038 l1_error: 0.000000\n",
      "ms_loss: 10.7458 ori_loss: 10.7458 l1_error: 0.000000\n",
      "ms_loss: 10.7263 ori_loss: 10.7263 l1_error: 0.000002\n",
      "ms_loss: 10.7328 ori_loss: 10.7328 l1_error: 0.000000\n",
      "ms_loss: 10.7089 ori_loss: 10.7089 l1_error: 0.000000\n",
      "ms_loss: 10.7920 ori_loss: 10.7920 l1_error: 0.000001\n",
      "ms_loss: 10.7554 ori_loss: 10.7554 l1_error: 0.000001\n",
      "ms_loss: 10.7446 ori_loss: 10.7446 l1_error: 0.000000\n",
      "ms_loss: 10.7007 ori_loss: 10.7007 l1_error: 0.000002\n",
      "ms_loss: 10.7871 ori_loss: 10.7871 l1_error: 0.000000\n",
      "ms_loss: 10.7191 ori_loss: 10.7191 l1_error: 0.000001\n",
      "ms_loss: 10.7571 ori_loss: 10.7571 l1_error: 0.000000\n",
      "ms_loss: 10.7503 ori_loss: 10.7503 l1_error: 0.000000\n",
      "ms_loss: 10.7569 ori_loss: 10.7569 l1_error: 0.000000\n",
      "ms_loss: 10.7321 ori_loss: 10.7321 l1_error: 0.000001\n",
      "ms_loss: 10.7659 ori_loss: 10.7659 l1_error: 0.000000\n",
      "ms_loss: 10.6841 ori_loss: 10.6841 l1_error: 0.000000\n",
      "ms_loss: 10.8024 ori_loss: 10.8024 l1_error: 0.000000\n",
      "ms_loss: 10.7800 ori_loss: 10.7800 l1_error: 0.000000\n",
      "ms_loss: 10.7033 ori_loss: 10.7033 l1_error: 0.000001\n",
      "ms_loss: 10.6773 ori_loss: 10.6773 l1_error: 0.000001\n",
      "ms_loss: 10.7466 ori_loss: 10.7466 l1_error: 0.000000\n",
      "ms_loss: 10.7190 ori_loss: 10.7190 l1_error: 0.000000\n",
      "ms_loss: 10.6255 ori_loss: 10.6255 l1_error: 0.000000\n",
      "ms_loss: 10.7796 ori_loss: 10.7796 l1_error: 0.000000\n",
      "ms_loss: 10.7785 ori_loss: 10.7785 l1_error: 0.000000\n",
      "ms_loss: 10.6727 ori_loss: 10.6727 l1_error: 0.000000\n",
      "ms_loss: 10.7507 ori_loss: 10.7507 l1_error: 0.000000\n",
      "ms_loss: 10.7448 ori_loss: 10.7448 l1_error: 0.000000\n",
      "ms_loss: 10.7142 ori_loss: 10.7142 l1_error: 0.000001\n",
      "ms_loss: 10.8497 ori_loss: 10.8497 l1_error: 0.000000\n",
      "ms_loss: 10.8731 ori_loss: 10.8731 l1_error: 0.000001\n",
      "ms_loss: 10.7117 ori_loss: 10.7117 l1_error: 0.000001\n",
      "ms_loss: 10.7330 ori_loss: 10.7330 l1_error: 0.000000\n",
      "ms_loss: 10.8038 ori_loss: 10.8038 l1_error: 0.000000\n",
      "ms_loss: 10.7120 ori_loss: 10.7120 l1_error: 0.000000\n",
      "ms_loss: 10.7866 ori_loss: 10.7866 l1_error: 0.000001\n",
      "ms_loss: 10.6974 ori_loss: 10.6974 l1_error: 0.000000\n",
      "ms_loss: 10.7403 ori_loss: 10.7403 l1_error: 0.000001\n",
      "ms_loss: 10.7043 ori_loss: 10.7043 l1_error: 0.000001\n",
      "ms_loss: 10.8285 ori_loss: 10.8285 l1_error: 0.000001\n",
      "ms_loss: 10.7652 ori_loss: 10.7652 l1_error: 0.000000\n",
      "ms_loss: 10.6297 ori_loss: 10.6297 l1_error: 0.000000\n",
      "ms_loss: 10.6798 ori_loss: 10.6798 l1_error: 0.000000\n",
      "ms_loss: 10.7054 ori_loss: 10.7054 l1_error: 0.000001\n",
      "ms_loss: 10.8473 ori_loss: 10.8473 l1_error: 0.000000\n",
      "ms_loss: 10.7666 ori_loss: 10.7666 l1_error: 0.000001\n",
      "ms_loss: 10.6871 ori_loss: 10.6871 l1_error: 0.000000\n",
      "ms_loss: 10.6617 ori_loss: 10.6617 l1_error: 0.000000\n",
      "ms_loss: 10.8253 ori_loss: 10.8253 l1_error: 0.000000\n",
      "ms_loss: 10.6879 ori_loss: 10.6879 l1_error: 0.000000\n",
      "ms_loss: 10.6420 ori_loss: 10.6420 l1_error: 0.000000\n",
      "ms_loss: 10.7316 ori_loss: 10.7316 l1_error: 0.000000\n",
      "ms_loss: 10.7088 ori_loss: 10.7088 l1_error: 0.000000\n",
      "ms_loss: 10.8095 ori_loss: 10.8095 l1_error: 0.000000\n",
      "ms_loss: 10.7505 ori_loss: 10.7505 l1_error: 0.000000\n",
      "ms_loss: 10.7349 ori_loss: 10.7349 l1_error: 0.000000\n",
      "ms_loss: 10.8229 ori_loss: 10.8229 l1_error: 0.000001\n",
      "ms_loss: 10.7697 ori_loss: 10.7697 l1_error: 0.000001\n",
      "ms_loss: 10.7560 ori_loss: 10.7560 l1_error: 0.000000\n",
      "ms_loss: 10.8151 ori_loss: 10.8151 l1_error: 0.000000\n",
      "ms_loss: 10.7284 ori_loss: 10.7284 l1_error: 0.000000\n",
      "ms_loss: 10.7548 ori_loss: 10.7548 l1_error: 0.000000\n",
      "ms_loss: 10.7252 ori_loss: 10.7252 l1_error: 0.000001\n",
      "ms_loss: 10.7324 ori_loss: 10.7324 l1_error: 0.000001\n",
      "ms_loss: 10.7365 ori_loss: 10.7365 l1_error: 0.000000\n",
      "ms_loss: 10.7654 ori_loss: 10.7654 l1_error: 0.000000\n",
      "ms_loss: 10.6383 ori_loss: 10.6383 l1_error: 0.000001\n",
      "ms_loss: 10.7546 ori_loss: 10.7546 l1_error: 0.000000\n",
      "ms_loss: 10.7405 ori_loss: 10.7405 l1_error: 0.000000\n",
      "ms_loss: 10.7991 ori_loss: 10.7991 l1_error: 0.000000\n",
      "ms_loss: 10.7305 ori_loss: 10.7305 l1_error: 0.000000\n",
      "ms_loss: 10.7117 ori_loss: 10.7117 l1_error: 0.000000\n",
      "ms_loss: 10.7626 ori_loss: 10.7626 l1_error: 0.000000\n",
      "ms_loss: 10.7494 ori_loss: 10.7494 l1_error: 0.000000\n",
      "ms_loss: 10.7787 ori_loss: 10.7787 l1_error: 0.000000\n",
      "ms_loss: 10.7793 ori_loss: 10.7793 l1_error: 0.000000\n",
      "ms_loss: 10.6958 ori_loss: 10.6958 l1_error: 0.000001\n",
      "ms_loss: 10.7518 ori_loss: 10.7518 l1_error: 0.000000\n",
      "ms_loss: 10.7133 ori_loss: 10.7133 l1_error: 0.000000\n",
      "ms_loss: 10.7901 ori_loss: 10.7901 l1_error: 0.000000\n",
      "ms_loss: 10.7656 ori_loss: 10.7656 l1_error: 0.000000\n",
      "ms_loss: 10.7033 ori_loss: 10.7033 l1_error: 0.000000\n",
      "ms_loss: 10.8142 ori_loss: 10.8142 l1_error: 0.000000\n",
      "ms_loss: 10.7528 ori_loss: 10.7528 l1_error: 0.000000\n",
      "ms_loss: 10.7733 ori_loss: 10.7733 l1_error: 0.000000\n",
      "ms_loss: 10.8357 ori_loss: 10.8357 l1_error: 0.000000\n",
      "ms_loss: 10.7000 ori_loss: 10.7000 l1_error: 0.000001\n",
      "ms_loss: 10.7200 ori_loss: 10.7200 l1_error: 0.000000\n",
      "ms_loss: 10.6564 ori_loss: 10.6564 l1_error: 0.000000\n",
      "ms_loss: 10.7937 ori_loss: 10.7937 l1_error: 0.000000\n",
      "ms_loss: 10.7230 ori_loss: 10.7230 l1_error: 0.000000\n",
      "ms_loss: 10.8071 ori_loss: 10.8071 l1_error: 0.000000\n",
      "ms_loss: 10.7291 ori_loss: 10.7291 l1_error: 0.000000\n",
      "ms_loss: 10.7573 ori_loss: 10.7573 l1_error: 0.000001\n",
      "ms_loss: 10.7728 ori_loss: 10.7728 l1_error: 0.000001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ms_loss: 10.7756 ori_loss: 10.7756 l1_error: 0.000000\n",
      "ms_loss: 10.7293 ori_loss: 10.7293 l1_error: 0.000000\n",
      "ms_loss: 10.7405 ori_loss: 10.7405 l1_error: 0.000000\n",
      "ms_loss: 10.6938 ori_loss: 10.6938 l1_error: 0.000000\n",
      "ms_loss: 10.7008 ori_loss: 10.7008 l1_error: 0.000001\n",
      "ms_loss: 10.7833 ori_loss: 10.7833 l1_error: 0.000000\n",
      "ms_loss: 10.7137 ori_loss: 10.7137 l1_error: 0.000001\n",
      "ms_loss: 10.8068 ori_loss: 10.8068 l1_error: 0.000000\n",
      "ms_loss: 10.6620 ori_loss: 10.6620 l1_error: 0.000000\n",
      "ms_loss: 10.8509 ori_loss: 10.8509 l1_error: 0.000000\n",
      "ms_loss: 10.7950 ori_loss: 10.7950 l1_error: 0.000000\n",
      "ms_loss: 10.7608 ori_loss: 10.7608 l1_error: 0.000001\n",
      "ms_loss: 10.7485 ori_loss: 10.7485 l1_error: 0.000000\n",
      "ms_loss: 10.6435 ori_loss: 10.6435 l1_error: 0.000001\n",
      "ms_loss: 10.6802 ori_loss: 10.6802 l1_error: 0.000000\n",
      "ms_loss: 10.7693 ori_loss: 10.7693 l1_error: 0.000000\n",
      "ms_loss: 10.6925 ori_loss: 10.6925 l1_error: 0.000000\n",
      "ms_loss: 10.7824 ori_loss: 10.7824 l1_error: 0.000001\n",
      "ms_loss: 10.6805 ori_loss: 10.6805 l1_error: 0.000000\n",
      "ms_loss: 10.6635 ori_loss: 10.6635 l1_error: 0.000000\n",
      "ms_loss: 10.7923 ori_loss: 10.7923 l1_error: 0.000000\n",
      "ms_loss: 10.7252 ori_loss: 10.7252 l1_error: 0.000000\n",
      "ms_loss: 10.7386 ori_loss: 10.7386 l1_error: 0.000000\n",
      "ms_loss: 10.7188 ori_loss: 10.7188 l1_error: 0.000001\n",
      "ms_loss: 10.7186 ori_loss: 10.7186 l1_error: 0.000000\n",
      "ms_loss: 10.6329 ori_loss: 10.6329 l1_error: 0.000000\n",
      "ms_loss: 10.6590 ori_loss: 10.6590 l1_error: 0.000000\n",
      "ms_loss: 10.7326 ori_loss: 10.7326 l1_error: 0.000000\n",
      "ms_loss: 10.7459 ori_loss: 10.7459 l1_error: 0.000001\n",
      "ms_loss: 10.7186 ori_loss: 10.7186 l1_error: 0.000000\n",
      "ms_loss: 10.7488 ori_loss: 10.7488 l1_error: 0.000000\n",
      "ms_loss: 10.8454 ori_loss: 10.8454 l1_error: 0.000000\n",
      "ms_loss: 10.7536 ori_loss: 10.7536 l1_error: 0.000001\n",
      "ms_loss: 10.7589 ori_loss: 10.7589 l1_error: 0.000000\n",
      "ms_loss: 10.7081 ori_loss: 10.7081 l1_error: 0.000001\n",
      "ms_loss: 10.7710 ori_loss: 10.7710 l1_error: 0.000000\n",
      "ms_loss: 10.8262 ori_loss: 10.8262 l1_error: 0.000000\n",
      "ms_loss: 10.6764 ori_loss: 10.6764 l1_error: 0.000000\n",
      "ms_loss: 10.7033 ori_loss: 10.7033 l1_error: 0.000001\n",
      "ms_loss: 10.7046 ori_loss: 10.7046 l1_error: 0.000000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "round_n = 1000\n",
    "for i in range(round_n):\n",
    "    x_rand = msnp.randn(x.shape)\n",
    "    y_rand = msnp.randn(y.shape).astype(ms.int32)\n",
    "    ms_loss = cross_entropy_v2(x + x_rand, y).asnumpy()\n",
    "    ori_loss = cross_entropy_ori(x + x_rand, y).asnumpy()\n",
    "    print(f\"ms_loss: {ms_loss:.5f}\\b ori_loss: {ori_loss:.5f}\\b l1_error: {np.abs(ms_loss - ori_loss):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c01bef8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = ops.stack((y_x, y), axis=0).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4fee29dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore import Tensor\n",
    "inds = Tensor([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f76e84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inds2 = Tensor([1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7416c5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Tensor([[1, 3, 5], [2, 4, 6], [3, 6, 8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8837c377",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = Tensor(((0,1), (1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86763dca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[2], dtype=Int64, value= [3, 6])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[[[0,1], [1,2]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d1628adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[1, 2, 2, 3], dtype=Int64, value=\n",
       "[[[[1, 3, 5],\n",
       "   [2, 4, 6]],\n",
       "  [[2, 4, 6],\n",
       "   [3, 6, 8]]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0c90f02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_gather = P.Gather()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b1ac0cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[3, 2, 2], dtype=Int64, value=\n",
       "[[[1, 3],\n",
       "  [3, 5]],\n",
       " [[2, 4],\n",
       "  [4, 6]],\n",
       " [[3, 6],\n",
       "  [6, 8]]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_gather(a, inds, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a4bfb044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[[1 3 5]\n",
      " [2 4 6]]\n",
      "[1 2]\n",
      "[[2 4 6]\n",
      " [3 6 8]]\n"
     ]
    }
   ],
   "source": [
    "for ind in inds:\n",
    "    print(ind)\n",
    "    print(a[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6ff7afb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_i = P.Concat(axis=0)([a[ind] for ind in inds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4b5a5ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[4, 2], dtype=Int64, value=\n",
       "[[1, 3],\n",
       " [2, 4],\n",
       " [2, 4],\n",
       " [3, 6]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "873dea83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[2, 2], dtype=Int64, value=\n",
       "[[2, 4],\n",
       " [3, 6]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[inds2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "49357580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(992, 1)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "728877c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[992, 1], dtype=Int32, value=\n",
       "[[1037],\n",
       " [4743],\n",
       " [7719],\n",
       " ...\n",
       " [-100],\n",
       " [-100],\n",
       " [-100]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "45b4936c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[1], dtype=Int32, value= [1037])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "10a0bb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = _gather_d(x, target_dim, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "668b219b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[992, 1], dtype=Float32, value=\n",
       "[[ 9.77265418e-01],\n",
       " [ 6.47866130e-01],\n",
       " [ 7.58817554e-01],\n",
       " ...\n",
       " [ 1.80141628e-02],\n",
       " [-6.06357396e-01],\n",
       " [ 5.73751628e-02]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "24bc65cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss2 = _gather_d2(x, target_dim, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "39b079f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[992, 1], dtype=Float32, value=\n",
       "[[ 9.77265418e-01],\n",
       " [ 6.47866130e-01],\n",
       " [ 7.58817554e-01],\n",
       " ...\n",
       " [ 1.80141628e-02],\n",
       " [-6.06357396e-01],\n",
       " [ 5.73751628e-02]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "75d72b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "_gather = P.Gather()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "76144780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[992, 30523], dtype=Float32, value=\n",
       "[[ 4.32402939e-02,  1.72222644e-01, -2.72406727e-01 ...  4.83820736e-01, -1.63055420e-01,  4.67659026e-01],\n",
       " [ 3.64985555e-01, -7.27577627e-01,  1.46366447e-01 ...  3.22835207e-01, -2.29929969e-01, -4.27669406e-01],\n",
       " [-5.47032356e-01,  1.43672317e-01, -2.06260383e-02 ...  2.38050371e-01, -2.35057652e-01,  2.05625534e-01],\n",
       " ...\n",
       " [ 6.30636215e-01,  2.92517602e-01, -3.93146753e-01 ... -4.71520424e-02,  4.35345054e-01, -7.17348099e-01],\n",
       " [ 5.66837251e-01, -2.24138916e-01, -8.57901335e-01 ... -4.89547193e-01, -4.35822397e-01, -1.05204093e+00],\n",
       " [ 6.53074265e-01, -2.94384956e-01, -6.31686032e-01 ...  2.86771655e-01, -6.35821670e-02, -2.80281723e-01]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1a76fd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] DEVICE(3434083,fffe37f64290,python):2023-10-10-11:02:47.117.680 [mindspore/ccsrc/plugin/device/ascend/hal/device/ascend_kernel_runtime.cc:675] TaskFailCallback] Execute TaskFailCallback failed. task_fail_info or current_graph_ is nullptr\n",
      "[ERROR] DEVICE(3434083,fffe37f64290,python):2023-10-10-11:02:47.117.782 [mindspore/ccsrc/plugin/device/ascend/hal/device/ascend_stream_manager.cc:186] SyncStream] Call runtime rtStreamSynchronize error.\n",
      "[ERROR] DEVICE(3434083,fffe37f64290,python):2023-10-10-11:02:47.117.796 [mindspore/ccsrc/plugin/device/ascend/hal/device/ascend_kernel_runtime.cc:1127] SyncStream] Sync default stream failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sync stream error!\n\n----------------------------------------------------\n- C++ Call Stack: (For framework developers)\n----------------------------------------------------\nmindspore/ccsrc/plugin/device/ascend/hal/device/ascend_device_address.cc:203 SyncStream\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3434083/2929392323.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_gather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/archiconda3/envs/yj-1.10-py37/lib/python3.7/site-packages/mindspore/ops/primitive.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshould_elim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_run_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/archiconda3/envs/yj-1.10-py37/lib/python3.7/site-packages/mindspore/common/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*arg, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_convert_python_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/archiconda3/envs/yj-1.10-py37/lib/python3.7/site-packages/mindspore/ops/primitive.py\u001b[0m in \u001b[0;36m_run_op\u001b[0;34m(obj, op_name, args)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_run_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0;34m\"\"\"Single op execution function supported by ge in PyNative mode.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_run_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sync stream error!\n\n----------------------------------------------------\n- C++ Call Stack: (For framework developers)\n----------------------------------------------------\nmindspore/ccsrc/plugin/device/ascend/hal/device/ascend_device_address.cc:203 SyncStream\n"
     ]
    }
   ],
   "source": [
    "loss = _gather(x, y1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e5d8c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[992, 992, 1], dtype=Float32, value=\n",
       "[[[ 9.77265418e-01],\n",
       "  [ 1.79878339e-01],\n",
       "  [ 5.28340578e-01],\n",
       "  ...\n",
       "  [ 0.00000000e+00],\n",
       "  [ 0.00000000e+00],\n",
       "  [ 0.00000000e+00]],\n",
       " [[ 1.43269628e-01],\n",
       "  [ 6.47866130e-01],\n",
       "  [ 1.35163844e+00],\n",
       "  ...\n",
       "  [-3.70816290e-01],\n",
       "  [-3.70816290e-01],\n",
       "  [-3.70816290e-01]],\n",
       " [[ 1.92740276e-01],\n",
       "  [ 9.82333869e-02],\n",
       "  [ 7.58817554e-01],\n",
       "  ...\n",
       "  [-7.43796885e-01],\n",
       "  [-7.43796885e-01],\n",
       "  [-7.43796885e-01]],\n",
       " ...\n",
       " [[ 8.45621347e-01],\n",
       "  [ 1.16952825e+00],\n",
       "  [ 1.81982666e-02],\n",
       "  ...\n",
       "  [-3.02402735e-01],\n",
       "  [-3.02402735e-01],\n",
       "  [-3.02402735e-01]],\n",
       " [[ 1.55460566e-01],\n",
       "  [ 4.58987981e-01],\n",
       "  [ 4.10465956e-01],\n",
       "  ...\n",
       "  [ 1.80141628e-02],\n",
       "  [ 1.80141628e-02],\n",
       "  [ 1.80141628e-02]],\n",
       " [[ 8.60769510e-01],\n",
       "  [ 8.50641847e-01],\n",
       "  [ 8.54853928e-01],\n",
       "  ...\n",
       "  [-6.06357396e-01],\n",
       "  [-6.06357396e-01],\n",
       "  [-6.06357396e-01]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03b799f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ops.ones(992, ms.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9a81b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[1], dtype=Float32, value= [ 8.83468628e+00])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss1(x, y, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4feb75c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import CrossEntropyLoss as torch_CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "903c14cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_torch = torch.from_numpy(x.asnumpy())\n",
    "y_torch = torch.from_numpy(y.asnumpy()).to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f889a9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_torch = torch_CrossEntropyLoss(label_smoothing=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83655854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.2473)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_torch(x_torch, y_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c686e831",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindformers.models.blip2.qformer import CrossEntropyLoss as CrossEntropyLoss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a6ee437",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss2 = CrossEntropyLoss2(label_smoothing=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5870e07e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[], dtype=Float32, value= 10.2473)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss2(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b738929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore.nn import CrossEntropyLoss as CrossEntropyLoss3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37124092",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss3 = CrossEntropyLoss3(label_smoothing=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a94ab992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(shape=[], dtype=Float32, value= 9.52042)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss3(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "50c4d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "coid = 10\n",
    "round_n = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f200cf2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.799058 10.799057\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.821322 10.821323\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.845504 10.845503\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.769189 10.76919\n",
      "False\n",
      "10.850328 10.8503275\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.828471 10.82847\n",
      "True\n",
      "True\n",
      "False\n",
      "10.862382 10.862383\n",
      "False\n",
      "10.874336 10.874337\n",
      "True\n",
      "True\n",
      "False\n",
      "10.80709 10.807089\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.800744 10.800745\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.829005 10.829004\n",
      "False\n",
      "10.806739 10.806741\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.802286 10.802287\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.843796 10.843795\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.77926 10.779259\n",
      "True\n",
      "False\n",
      "10.809778 10.809777\n",
      "False\n",
      "10.807304 10.807303\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.878033 10.878031\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.826295 10.826293\n",
      "True\n",
      "False\n",
      "10.833531 10.833532\n",
      "False\n",
      "10.806616 10.806615\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.839711 10.83971\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.823767 10.823768\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.8516445 10.851645\n",
      "True\n",
      "True\n",
      "False\n",
      "10.8188715 10.818871\n",
      "True\n",
      "True\n",
      "False\n",
      "10.850641 10.850639\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.781737 10.781738\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.856488 10.856489\n",
      "True\n",
      "False\n",
      "10.842227 10.842226\n",
      "True\n",
      "True\n",
      "False\n",
      "10.801975 10.801974\n",
      "True\n",
      "True\n",
      "False\n",
      "10.822468 10.822467\n",
      "True\n",
      "False\n",
      "10.84619 10.846191\n",
      "False\n",
      "10.855038 10.855039\n",
      "False\n",
      "10.838585 10.838586\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.754832 10.754833\n",
      "True\n",
      "True\n",
      "False\n",
      "10.809574 10.809573\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.813017 10.813016\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.845197 10.845198\n",
      "True\n",
      "False\n",
      "10.804374 10.804375\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.793142 10.79314\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.805525 10.805524\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.892603 10.892605\n",
      "False\n",
      "10.844223 10.844224\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.819459 10.81946\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.822751 10.822752\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.874336 10.874337\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.8266115 10.826612\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.827504 10.827503\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.789008 10.789009\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.81267 10.812669\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.838401 10.8384\n",
      "False\n",
      "10.832967 10.832968\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.860105 10.860104\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.829014 10.829015\n",
      "True\n",
      "True\n",
      "False\n",
      "10.802657 10.802656\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.846454 10.846453\n",
      "True\n",
      "False\n",
      "10.857242 10.857243\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.852455 10.852453\n",
      "True\n",
      "False\n",
      "10.8326 10.832599\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.843965 10.843966\n",
      "False\n",
      "10.804333 10.804334\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.835218 10.835219\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.832654 10.832655\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.8579 10.857901\n",
      "True\n",
      "True\n",
      "False\n",
      "10.812747 10.812746\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.788355 10.788356\n",
      "False\n",
      "10.847665 10.847664\n",
      "True\n",
      "True\n",
      "False\n",
      "10.830462 10.8304615\n",
      "False\n",
      "10.811608 10.811609\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.81504 10.815039\n",
      "False\n",
      "10.871356 10.871355\n",
      "False\n",
      "10.86694 10.866939\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.810238 10.810237\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.877425 10.877427\n",
      "True\n",
      "False\n",
      "10.838108 10.838109\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.807682 10.807683\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.806063 10.806064\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.813337 10.813338\n",
      "False\n",
      "10.842632 10.842633\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.821169 10.821168\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.837656 10.837654\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.8227415 10.822742\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.8614435 10.861444\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.777556 10.777555\n",
      "False\n",
      "10.7652645 10.765265\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.871507 10.871508\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.779525 10.779526\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.857815 10.857813\n",
      "False\n",
      "10.765772 10.765773\n",
      "True\n",
      "True\n",
      "False\n",
      "10.8207245 10.820725\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.779724 10.779722\n",
      "False\n",
      "10.821367 10.821368\n",
      "True\n",
      "True\n",
      "False\n",
      "10.823161 10.82316\n",
      "False\n",
      "10.802082 10.802081\n",
      "True\n",
      "True\n",
      "False\n",
      "10.853546 10.853545\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.7659025 10.765903\n",
      "True\n",
      "False\n",
      "10.853184 10.853183\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.839461 10.839462\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.83599 10.835989\n",
      "False\n",
      "10.874953 10.874954\n",
      "False\n",
      "10.784208 10.784209\n",
      "False\n",
      "10.87756 10.877559\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.779955 10.779954\n",
      "True\n",
      "True\n",
      "False\n",
      "10.81781 10.817811\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.846549 10.84655\n",
      "True\n",
      "True\n",
      "False\n",
      "10.884855 10.884854\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.857868 10.857867\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.760126 10.760127\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.797076 10.797077\n",
      "True\n",
      "True\n",
      "False\n",
      "10.864837 10.864836\n",
      "True\n",
      "False\n",
      "10.91973 10.919731\n",
      "False\n",
      "10.807105 10.807106\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.826305 10.826307\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.822951 10.822952\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.789868 10.789869\n",
      "True\n",
      "False\n",
      "10.852113 10.852114\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.768933 10.768932\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.805665 10.805664\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.786296 10.786297\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.816981 10.816982\n",
      "False\n",
      "10.859765 10.859764\n",
      "True\n",
      "False\n",
      "10.885462 10.885463\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.859509 10.8595085\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.899832 10.899831\n",
      "True\n",
      "True\n",
      "False\n",
      "10.836693 10.836694\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.808965 10.808966\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.753982 10.753983\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.816129 10.816128\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.856021 10.856022\n",
      "True\n",
      "False\n",
      "10.810571 10.810572\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.802338 10.802339\n",
      "True\n",
      "False\n",
      "10.828176 10.8281765\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.771203 10.771204\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.815579 10.81558\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.867423 10.867424\n",
      "False\n",
      "10.799585 10.799586\n",
      "True\n",
      "True\n",
      "False\n",
      "10.872802 10.872803\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.820222 10.820223\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.847171 10.84717\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.785963 10.785965\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.817882 10.817881\n",
      "True\n",
      "True\n",
      "False\n",
      "10.785975 10.7859745\n",
      "False\n",
      "10.813379 10.813378\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.876143 10.876144\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.803492 10.803493\n",
      "False\n",
      "10.789248 10.7892475\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.792505 10.792506\n",
      "True\n",
      "False\n",
      "10.808903 10.808905\n",
      "True\n",
      "True\n",
      "False\n",
      "10.799087 10.799088\n",
      "False\n",
      "10.811699 10.8117\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.841236 10.841235\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.859088 10.859087\n",
      "True\n",
      "False\n",
      "10.848721 10.84872\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.873672 10.873671\n",
      "False\n",
      "10.829168 10.829169\n",
      "True\n",
      "True\n",
      "False\n",
      "10.854684 10.854685\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.799148 10.799147\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.889518 10.889517\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.808159 10.80816\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "10.826852 10.826851\n",
      "False\n",
      "10.825918 10.825919\n",
      "False\n",
      "10.816288 10.816287\n",
      "False\n",
      "10.882231 10.882232\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.843797 10.843796\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "10.821589 10.82159\n",
      "True\n",
      "True\n",
      "False\n",
      "10.799782 10.799781\n",
      "True\n",
      "coincide rate: 0.834\n"
     ]
    }
   ],
   "source": [
    "coid = 0\n",
    "round_n = 1000\n",
    "for i in range(round_n):\n",
    "    x = msnp.randn(992, 30523)\n",
    "    y = msnp.randn(992).astype(ms.int32)\n",
    "    l2 = loss2(x, y)\n",
    "    l3 = loss3(x, y)\n",
    "    if l2 == l3:\n",
    "        print(True)\n",
    "        coid += 1\n",
    "    else:\n",
    "        print(False)\n",
    "        print(l2, l3)\n",
    "\n",
    "print(f\"coincide rate: {(coid/round_n):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20c06be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yj-1.10-py37",
   "language": "python",
   "name": "yj-1.10-py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
